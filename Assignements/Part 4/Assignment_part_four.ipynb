{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/michalis0/DataScience_and_MachineLearning/blob/master/Assignements/Part%204/Assignment_part_four.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kZu-7QbP9muh"
   },
   "source": [
    "DSML investigation:\n",
    "\n",
    "You are part of the Suisse Impossible Mission Force, or SIMF for short. You need to uncover a rogue agent that is trying to steal sensitive information.\n",
    "\n",
    "Your mission, should you choose to accept it, is to find that agent before stealing any classified information. Good luck!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DyL7WNdV9sWV"
   },
   "source": [
    "# Assignement part four\n",
    "### Due 22.10\n",
    "#### Identifying the suspects credit score\n",
    "We received informations that the rogue agent has a good credit score.\n",
    "\n",
    "Our spies at SIMF have managed to collect financial information relating to our suspects as well as a training dataset.\n",
    "\n",
    "Create a Neural Network over the training dataset `df` to identify which of the suspects have a good Credit_Mix\n",
    "\n",
    "\n",
    "## Getting to know our data\n",
    "\n",
    "* Age: a users age\n",
    "* Occupation: a users employment field\n",
    "* Annual_Income: a users annual income\n",
    "* Monthly_Inh_Salary: the calculated salary received by a given user on a monthly basis\n",
    "* Num_Bank_Accounts: the number of bank accounts possessed by a given user\n",
    "* Num_Credit_Cards: the number of credit card given user possesses\n",
    "* Interest_Rate: The interest rate on those cards (if multiple then its the average)\n",
    "* Num_of_Loans: The number of loans of each user\n",
    "* Delay_from_due_date: payment tardiness of user\n",
    "* Num_of_Delayed_Payment: the count of delayed payments\n",
    "* Changed_Credit_Limit: NaN\n",
    "* Num_Credit_Inquiries: NaN\n",
    "* Credit_Mix: The users credit score\n",
    "* Outsting_Debt: Outstanding debt\n",
    "* Credit_Utilization_Ratio: the percentage of borrowed money over borrowing allowance\n",
    "* Payment_of_Min_Amount: does the user usually pay the minimal amount (categorical)\n",
    "* Total_EMI_per_month: Monthly repayments to be made\n",
    "* Amount_invested_monthly: The amout put in an investment fun by the user on a monthly basis\n",
    "* Payment_Behaviour: the users payment behavior (categorical)\n",
    "* Monthly_Balance: The users end of the month balance\n",
    "* AutoLoan: If the user has an active loan for their vehicule\n",
    "* Credit-BuilderLoan: If the user has a loan to increase their credit score\n",
    "* DebtConsolidationLoan, HomeEquityLoan, MortgageLoan, NotSpecified, PaydayLoan, PersonalLoan, StudentLoan: different types of loans(categorical features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "XHhI95r5-tyD"
   },
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"https://raw.githubusercontent.com/michalis0/DataScience_and_MachineLearning/master/Assignements/Part%204/data/train_classification.csv\", index_col='Unnamed: 0').dropna()\n",
    "suspects = pd.read_csv(\"https://raw.githubusercontent.com/michalis0/DataScience_and_MachineLearning/master/Assignements/Part%204/data/suspects.csv\", index_col='Unnamed: 0').dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 29223 entries, 0 to 49998\n",
      "Data columns (total 29 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   Age                       29223 non-null  int64  \n",
      " 1   Occupation                29223 non-null  object \n",
      " 2   Annual_Income             29223 non-null  float64\n",
      " 3   Monthly_Inh_Salary        29223 non-null  float64\n",
      " 4   Num_Bank_Accounts         29223 non-null  int64  \n",
      " 5   Num_Credit_Card           29223 non-null  int64  \n",
      " 6   Interest_Rate             29223 non-null  int64  \n",
      " 7   Num_of_Loan               29223 non-null  int64  \n",
      " 8   Delay_from_due_date       29223 non-null  int64  \n",
      " 9   Num_of_Delayed_Payment    29223 non-null  int64  \n",
      " 10  Changed_Credit_Limit      29223 non-null  float64\n",
      " 11  Num_Credit_Inquiries      29223 non-null  float64\n",
      " 12  Credit_Mix                29223 non-null  object \n",
      " 13  Outsting_Debt             29223 non-null  float64\n",
      " 14  Credit_Utilization_Ratio  29223 non-null  float64\n",
      " 15  Payment_of_Min_Amount     29223 non-null  object \n",
      " 16  Total_EMI_per_month       29223 non-null  float64\n",
      " 17  Amount_invested_monthly   29223 non-null  float64\n",
      " 18  Payment_Behaviour         29223 non-null  object \n",
      " 19  Monthly_Balance           29223 non-null  float64\n",
      " 20  AutoLoan                  29223 non-null  int64  \n",
      " 21  Credit-BuilderLoan        29223 non-null  int64  \n",
      " 22  DebtConsolidationLoan     29223 non-null  int64  \n",
      " 23  HomeEquityLoan            29223 non-null  int64  \n",
      " 24  MortgageLoan              29223 non-null  int64  \n",
      " 25  NotSpecified              29223 non-null  int64  \n",
      " 26  PaydayLoan                29223 non-null  int64  \n",
      " 27  PersonalLoan              29223 non-null  int64  \n",
      " 28  StudentLoan               29223 non-null  int64  \n",
      "dtypes: float64(9), int64(16), object(4)\n",
      "memory usage: 6.7+ MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 714 entries, 0 to 1237\n",
      "Data columns (total 43 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   Age                       714 non-null    float64\n",
      " 1   Annual_Income             714 non-null    float64\n",
      " 2   Monthly_Inh_Salary        714 non-null    float64\n",
      " 3   Num_Bank_Accounts         714 non-null    float64\n",
      " 4   Num_Credit_Card           714 non-null    float64\n",
      " 5   Interest_Rate             714 non-null    float64\n",
      " 6   Num_of_Loan               714 non-null    float64\n",
      " 7   Delay_from_due_date       714 non-null    float64\n",
      " 8   Num_of_Delayed_Payment    714 non-null    float64\n",
      " 9   Changed_Credit_Limit      714 non-null    float64\n",
      " 10  Num_Credit_Inquiries      714 non-null    float64\n",
      " 11  Outsting_Debt             714 non-null    float64\n",
      " 12  Credit_Utilization_Ratio  714 non-null    float64\n",
      " 13  Payment_of_Min_Amount     714 non-null    int64  \n",
      " 14  Total_EMI_per_month       714 non-null    float64\n",
      " 15  Amount_invested_monthly   714 non-null    float64\n",
      " 16  Payment_Behaviour         714 non-null    int64  \n",
      " 17  Monthly_Balance           714 non-null    float64\n",
      " 18  AutoLoan                  714 non-null    float64\n",
      " 19  Credit-BuilderLoan        714 non-null    float64\n",
      " 20  DebtConsolidationLoan     714 non-null    float64\n",
      " 21  HomeEquityLoan            714 non-null    float64\n",
      " 22  MortgageLoan              714 non-null    float64\n",
      " 23  NotSpecified              714 non-null    float64\n",
      " 24  PaydayLoan                714 non-null    float64\n",
      " 25  PersonalLoan              714 non-null    float64\n",
      " 26  StudentLoan               714 non-null    float64\n",
      " 27  Occupation_Accountant     714 non-null    float64\n",
      " 28  Occupation_Architect      714 non-null    float64\n",
      " 29  Occupation_Developer      714 non-null    float64\n",
      " 30  Occupation_Doctor         714 non-null    float64\n",
      " 31  Occupation_Engineer       714 non-null    float64\n",
      " 32  Occupation_Entrepreneur   714 non-null    float64\n",
      " 33  Occupation_Journalist     714 non-null    float64\n",
      " 34  Occupation_Lawyer         714 non-null    float64\n",
      " 35  Occupation_Manager        714 non-null    float64\n",
      " 36  Occupation_Mechanic       714 non-null    float64\n",
      " 37  Occupation_MediaManager   714 non-null    float64\n",
      " 38  Occupation_Musician       714 non-null    float64\n",
      " 39  Occupation_Scientist      714 non-null    float64\n",
      " 40  Occupation_Teacher        714 non-null    float64\n",
      " 41  Occupation_Writer         714 non-null    float64\n",
      " 42  userID                    714 non-null    int64  \n",
      "dtypes: float64(40), int64(3)\n",
      "memory usage: 245.4 KB\n",
      "0     Good\n",
      "1     Good\n",
      "3     Good\n",
      "5     Good\n",
      "8     Good\n",
      "9     Good\n",
      "10    Good\n",
      "12    Good\n",
      "14    Good\n",
      "15    Good\n",
      "Name: Credit_Mix, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df.info()\n",
    "suspects.info()\n",
    "print(df['Credit_Mix'].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Credit_Mix\n",
       "Standard    13421\n",
       "Good         8963\n",
       "Bad          6839\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Credit_Mix\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LZENObtyefVk"
   },
   "source": [
    "# 1. Preparing the data\n",
    "## 1.1 Data cleaning\n",
    " Perform OHE over the \"Occupation\" feature\n",
    "\n",
    " Then, perform LE over Payment_of_Min_Amount and Payment_Behaviour\n",
    "\n",
    " _hint: As we will be testing only one model no need to define a pipeline_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "JGVrLNJTefVk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Age  Annual_Income  Monthly_Inh_Salary  Num_Bank_Accounts  Num_Credit_Card  \\\n",
      "0   23       19114.12         1824.843333                  3                4   \n",
      "1   24       19114.12         1824.843333                  3                4   \n",
      "3   24       19114.12         4182.004291                  3                4   \n",
      "5   28       34847.84         3037.986667                  2                4   \n",
      "8   35      143162.64         4182.004291                  1                5   \n",
      "\n",
      "   Interest_Rate  Num_of_Loan  Delay_from_due_date  Num_of_Delayed_Payment  \\\n",
      "0              3            4                    3                       7   \n",
      "1              3            4                    3                       9   \n",
      "3              3            4                    4                       5   \n",
      "5              6            1                    3                       3   \n",
      "8              8            3                    8                    1942   \n",
      "\n",
      "   Changed_Credit_Limit  ...  Occupation_Entrepreneur Occupation_Journalist  \\\n",
      "0                 11.27  ...                      0.0                   0.0   \n",
      "1                 13.27  ...                      0.0                   0.0   \n",
      "3                 11.27  ...                      0.0                   0.0   \n",
      "5                  5.42  ...                      0.0                   0.0   \n",
      "8                  7.10  ...                      0.0                   0.0   \n",
      "\n",
      "   Occupation_Lawyer  Occupation_Manager Occupation_Mechanic  \\\n",
      "0                0.0                 0.0                 0.0   \n",
      "1                0.0                 0.0                 0.0   \n",
      "3                0.0                 0.0                 0.0   \n",
      "5                0.0                 0.0                 0.0   \n",
      "8                0.0                 0.0                 0.0   \n",
      "\n",
      "   Occupation_MediaManager  Occupation_Musician Occupation_Scientist  \\\n",
      "0                      0.0                  0.0                  1.0   \n",
      "1                      0.0                  0.0                  1.0   \n",
      "3                      0.0                  0.0                  1.0   \n",
      "5                      0.0                  0.0                  0.0   \n",
      "8                      0.0                  0.0                  0.0   \n",
      "\n",
      "   Occupation_Teacher  Occupation_Writer  \n",
      "0                 0.0                0.0  \n",
      "1                 0.0                0.0  \n",
      "3                 0.0                0.0  \n",
      "5                 1.0                0.0  \n",
      "8                 0.0                0.0  \n",
      "\n",
      "[5 rows x 43 columns]\n",
      "0         No\n",
      "1         No\n",
      "3         No\n",
      "5         No\n",
      "8         No\n",
      "        ... \n",
      "49990     No\n",
      "49992    Yes\n",
      "49993    Yes\n",
      "49997     No\n",
      "49998     No\n",
      "Name: Payment_of_Min_Amount, Length: 29223, dtype: object 0          LowspentSmallvaluepayments\n",
      "1        HighspentMediumvaluepayments\n",
      "3        HighspentMediumvaluepayments\n",
      "5          LowspentLargevaluepayments\n",
      "8         LowspentMediumvaluepayments\n",
      "                     ...             \n",
      "49990      LowspentLargevaluepayments\n",
      "49992      LowspentSmallvaluepayments\n",
      "49993      LowspentSmallvaluepayments\n",
      "49997      LowspentLargevaluepayments\n",
      "49998     HighspentSmallvaluepayments\n",
      "Name: Payment_Behaviour, Length: 29223, dtype: object\n",
      "0        1\n",
      "1        1\n",
      "3        1\n",
      "5        1\n",
      "8        1\n",
      "        ..\n",
      "49990    1\n",
      "49992    2\n",
      "49993    2\n",
      "49997    1\n",
      "49998    1\n",
      "Name: Payment_of_Min_Amount, Length: 29223, dtype: int64 0        6\n",
      "1        2\n",
      "3        2\n",
      "5        4\n",
      "8        5\n",
      "        ..\n",
      "49990    4\n",
      "49992    6\n",
      "49993    6\n",
      "49997    4\n",
      "49998    3\n",
      "Name: Payment_Behaviour, Length: 29223, dtype: int64\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 29223 entries, 0 to 49998\n",
      "Data columns (total 43 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   Age                       29223 non-null  int64  \n",
      " 1   Annual_Income             29223 non-null  float64\n",
      " 2   Monthly_Inh_Salary        29223 non-null  float64\n",
      " 3   Num_Bank_Accounts         29223 non-null  int64  \n",
      " 4   Num_Credit_Card           29223 non-null  int64  \n",
      " 5   Interest_Rate             29223 non-null  int64  \n",
      " 6   Num_of_Loan               29223 non-null  int64  \n",
      " 7   Delay_from_due_date       29223 non-null  int64  \n",
      " 8   Num_of_Delayed_Payment    29223 non-null  int64  \n",
      " 9   Changed_Credit_Limit      29223 non-null  float64\n",
      " 10  Num_Credit_Inquiries      29223 non-null  float64\n",
      " 11  Credit_Mix                29223 non-null  object \n",
      " 12  Outsting_Debt             29223 non-null  float64\n",
      " 13  Credit_Utilization_Ratio  29223 non-null  float64\n",
      " 14  Payment_of_Min_Amount     29223 non-null  int64  \n",
      " 15  Total_EMI_per_month       29223 non-null  float64\n",
      " 16  Amount_invested_monthly   29223 non-null  float64\n",
      " 17  Payment_Behaviour         29223 non-null  int64  \n",
      " 18  Monthly_Balance           29223 non-null  float64\n",
      " 19  AutoLoan                  29223 non-null  int64  \n",
      " 20  Credit-BuilderLoan        29223 non-null  int64  \n",
      " 21  DebtConsolidationLoan     29223 non-null  int64  \n",
      " 22  HomeEquityLoan            29223 non-null  int64  \n",
      " 23  MortgageLoan              29223 non-null  int64  \n",
      " 24  NotSpecified              29223 non-null  int64  \n",
      " 25  PaydayLoan                29223 non-null  int64  \n",
      " 26  PersonalLoan              29223 non-null  int64  \n",
      " 27  StudentLoan               29223 non-null  int64  \n",
      " 28  Occupation_Accountant     29223 non-null  float64\n",
      " 29  Occupation_Architect      29223 non-null  float64\n",
      " 30  Occupation_Developer      29223 non-null  float64\n",
      " 31  Occupation_Doctor         29223 non-null  float64\n",
      " 32  Occupation_Engineer       29223 non-null  float64\n",
      " 33  Occupation_Entrepreneur   29223 non-null  float64\n",
      " 34  Occupation_Journalist     29223 non-null  float64\n",
      " 35  Occupation_Lawyer         29223 non-null  float64\n",
      " 36  Occupation_Manager        29223 non-null  float64\n",
      " 37  Occupation_Mechanic       29223 non-null  float64\n",
      " 38  Occupation_MediaManager   29223 non-null  float64\n",
      " 39  Occupation_Musician       29223 non-null  float64\n",
      " 40  Occupation_Scientist      29223 non-null  float64\n",
      " 41  Occupation_Teacher        29223 non-null  float64\n",
      " 42  Occupation_Writer         29223 non-null  float64\n",
      "dtypes: float64(24), int64(18), object(1)\n",
      "memory usage: 9.8+ MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nathan/.local/lib/python3.10/site-packages/sklearn/preprocessing/_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Your code here:\n",
    "ohe = OneHotEncoder(sparse= False)\n",
    "df_fitted = ohe.fit_transform(df[['Occupation']])\n",
    "categories = ohe.get_feature_names_out(input_features=['Occupation'])\n",
    "df_encoded = pd.DataFrame(df_fitted, columns=categories, index= df.index)\n",
    "# Replace the 'Occupation' column with the one-hot encoded columns\n",
    "df_encoded = pd.concat([df.drop(['Occupation'], axis=1), df_encoded], axis=1)\n",
    "print(df_encoded.head())\n",
    "le = LabelEncoder()\n",
    "print(df['Payment_of_Min_Amount'], df['Payment_Behaviour'])\n",
    "df_encoded['Payment_of_Min_Amount'] = le.fit_transform(df_encoded['Payment_of_Min_Amount'])\n",
    "df_encoded['Payment_Behaviour'] = le.fit_transform(df_encoded['Payment_Behaviour'])\n",
    "\n",
    "print(df_encoded['Payment_of_Min_Amount'], df_encoded['Payment_Behaviour'])\n",
    "\n",
    "df_encoded.info()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nfPnhUxAefVl"
   },
   "source": [
    "## 1.2 Dataset splitting\n",
    "\n",
    "Split the dataset in two, first X with your independent features and then y with the dependent feature **CreditMix**.\n",
    "\n",
    "Then perform :\n",
    "* OneHotEncoding over the **CreditMix** feature.\n",
    "* A MinMaxScaller over the independent features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 439
    },
    "id": "J-mentarefVm",
    "outputId": "f955a93e-5e45-4f91-d183-0c3aa19e0caf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Credit_Mix_Bad  Credit_Mix_Good  Credit_Mix_Standard\n",
      "0                 0.0              1.0                  0.0\n",
      "1                 0.0              1.0                  0.0\n",
      "3                 0.0              1.0                  0.0\n",
      "5                 0.0              1.0                  0.0\n",
      "8                 0.0              1.0                  0.0\n",
      "...               ...              ...                  ...\n",
      "49990             0.0              1.0                  0.0\n",
      "49992             1.0              0.0                  0.0\n",
      "49993             1.0              0.0                  0.0\n",
      "49997             0.0              1.0                  0.0\n",
      "49998             0.0              1.0                  0.0\n",
      "\n",
      "[29223 rows x 3 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nathan/.local/lib/python3.10/site-packages/sklearn/preprocessing/_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Age  Annual_Income  Monthly_Inh_Salary  Num_Bank_Accounts  \\\n",
      "0      0.057177       0.000502            0.102087           0.002242   \n",
      "1      0.057287       0.000502            0.102087           0.002242   \n",
      "3      0.057287       0.000502            0.260275           0.002242   \n",
      "5      0.057724       0.001154            0.183501           0.001682   \n",
      "8      0.058489       0.005643            0.260275           0.001121   \n",
      "...         ...            ...                 ...                ...   \n",
      "49990  0.060129       0.001251            0.187462           0.001121   \n",
      "49992  0.057833       0.000539            0.109138           0.006166   \n",
      "49993  0.057833       0.000539            0.109138           0.006166   \n",
      "49997  0.057396       0.001352            0.205072           0.002803   \n",
      "49998  0.057396       0.001352            0.260275           0.002803   \n",
      "\n",
      "       Num_Credit_Card  Interest_Rate  Num_of_Loan  Delay_from_due_date  \\\n",
      "0             0.002668       0.000345     0.065163             0.111111   \n",
      "1             0.002668       0.000345     0.065163             0.111111   \n",
      "3             0.002668       0.000345     0.065163             0.125000   \n",
      "5             0.002668       0.000862     0.063283             0.111111   \n",
      "8             0.003336       0.001207     0.064536             0.180556   \n",
      "...                ...            ...          ...                  ...   \n",
      "49990         0.002668       0.733184     0.064536             0.166667   \n",
      "49992         0.005337       0.004829     0.065789             0.527778   \n",
      "49993         0.005337       0.004829     0.065789             0.527778   \n",
      "49997         0.004003       0.001035     0.063910             0.388889   \n",
      "49998         0.004003       0.001035     0.063910             0.361111   \n",
      "\n",
      "       Num_of_Delayed_Payment  Changed_Credit_Limit  ...  \\\n",
      "0                    0.002301              0.417727  ...   \n",
      "1                    0.002761              0.464875  ...   \n",
      "3                    0.001841              0.417727  ...   \n",
      "5                    0.001381              0.279821  ...   \n",
      "8                    0.447538              0.319425  ...   \n",
      "...                       ...                   ...  ...   \n",
      "49990                0.003451              0.278878  ...   \n",
      "49992                0.006443              0.583687  ...   \n",
      "49993                0.006443              0.583687  ...   \n",
      "49997                0.001841              0.470297  ...   \n",
      "49998                0.002071              0.423149  ...   \n",
      "\n",
      "       Occupation_Entrepreneur  Occupation_Journalist  Occupation_Lawyer  \\\n",
      "0                          0.0                    0.0                0.0   \n",
      "1                          0.0                    0.0                0.0   \n",
      "3                          0.0                    0.0                0.0   \n",
      "5                          0.0                    0.0                0.0   \n",
      "8                          0.0                    0.0                0.0   \n",
      "...                        ...                    ...                ...   \n",
      "49990                      0.0                    0.0                0.0   \n",
      "49992                      0.0                    0.0                0.0   \n",
      "49993                      0.0                    0.0                0.0   \n",
      "49997                      0.0                    0.0                0.0   \n",
      "49998                      0.0                    0.0                0.0   \n",
      "\n",
      "       Occupation_Manager  Occupation_Mechanic  Occupation_MediaManager  \\\n",
      "0                     0.0                  0.0                      0.0   \n",
      "1                     0.0                  0.0                      0.0   \n",
      "3                     0.0                  0.0                      0.0   \n",
      "5                     0.0                  0.0                      0.0   \n",
      "8                     0.0                  0.0                      0.0   \n",
      "...                   ...                  ...                      ...   \n",
      "49990                 0.0                  0.0                      0.0   \n",
      "49992                 0.0                  0.0                      0.0   \n",
      "49993                 0.0                  0.0                      0.0   \n",
      "49997                 0.0                  1.0                      0.0   \n",
      "49998                 0.0                  1.0                      0.0   \n",
      "\n",
      "       Occupation_Musician  Occupation_Scientist  Occupation_Teacher  \\\n",
      "0                      0.0                   1.0                 0.0   \n",
      "1                      0.0                   1.0                 0.0   \n",
      "3                      0.0                   1.0                 0.0   \n",
      "5                      0.0                   0.0                 1.0   \n",
      "8                      0.0                   0.0                 0.0   \n",
      "...                    ...                   ...                 ...   \n",
      "49990                  0.0                   0.0                 0.0   \n",
      "49992                  0.0                   0.0                 0.0   \n",
      "49993                  0.0                   0.0                 0.0   \n",
      "49997                  0.0                   0.0                 0.0   \n",
      "49998                  0.0                   0.0                 0.0   \n",
      "\n",
      "       Occupation_Writer  \n",
      "0                    0.0  \n",
      "1                    0.0  \n",
      "3                    0.0  \n",
      "5                    0.0  \n",
      "8                    0.0  \n",
      "...                  ...  \n",
      "49990                1.0  \n",
      "49992                0.0  \n",
      "49993                0.0  \n",
      "49997                0.0  \n",
      "49998                0.0  \n",
      "\n",
      "[29223 rows x 42 columns]\n"
     ]
    }
   ],
   "source": [
    "# Your code here:\n",
    "X = pd.DataFrame(df_encoded.drop(columns=['Credit_Mix']))\n",
    "y_global = pd.DataFrame(df_encoded['Credit_Mix'])\n",
    "#print(y.index)\n",
    "y_fitted = ohe.fit_transform(y_global[['Credit_Mix']])\n",
    "categories_credit_mix = ohe.get_feature_names_out(input_features=['Credit_Mix'])\n",
    "y = pd.DataFrame(y_fitted, columns=categories_credit_mix, index= y_global.index)\n",
    "# Replace the 'Occupation' column with the one-hot encoded columns\n",
    "#y = pd.concat([y.drop(['Credit_mix'], axis=1), y], axis=1)\n",
    "print(y)\n",
    "\n",
    "#Define the scaler\n",
    "scaler = MinMaxScaler()\n",
    "#Fit the scaler\n",
    "scaler.fit_transform(X)\n",
    "\n",
    "X.loc[:,:] = scaler.transform(X)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 Train Test splitting\n",
    "Now split the data in X_train, X_test, y_train, y_test, \n",
    "\n",
    "You can use test_size = 0.2 and a random_state of 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "yotEvoAxefVn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23378, 42) (5845, 42) (23378, 3) (5845, 3)\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 final touches\n",
    "Convert your datasets to `Torch tensors` of type `torch.float`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "2KS_U8stefVo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([23378, 42]) torch.Size([23378, 3]) torch.Size([5845, 3]) torch.Size([5845, 42])\n"
     ]
    }
   ],
   "source": [
    "#Your code here:\n",
    "X_train = torch.tensor(X_train.values, dtype=torch.float)\n",
    "y_train = torch.tensor(y_train.values, dtype=torch.float)\n",
    "X_test = torch.tensor(X_test.values, dtype=torch.float)\n",
    "y_test = torch.tensor(y_test.values, dtype=torch.float)\n",
    "print(X_train.size(), y_train.size(), y_test.size(), X_test.size())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Model preparation:\n",
    "\n",
    "## 2.1 Define a Neural network model and instantiate it.\n",
    "You can set the number of neurons to 150."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "8qqRoocVefVp"
   },
   "outputs": [],
   "source": [
    "# Define a neural network class here:\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, D_in, H1, D_out):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.linear1 = nn.Linear(D_in, H1)        # Linear transformation for hidden layer\n",
    "        self.linear2 = nn.Linear(H1, D_out)       # Linear transformation for output layer\n",
    "        self.activation = nn.ReLU()               # Activation function for hidden layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        y_pred = self.activation(self.linear1(x))   # Hidden layer: linear transformation + ReLU\n",
    "        y_pred = self.linear2(y_pred)               # Output layer: linear transformation\n",
    "        return y_pred\n",
    "\n",
    "# Define the input and output sizes\n",
    "D_in =  X_train.shape[1]\n",
    "D_out =  y_train.shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "cx-3yvp5efVp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (linear1): Linear(in_features=42, out_features=150, bias=True)\n",
      "  (linear2): Linear(in_features=150, out_features=3, bias=True)\n",
      "  (activation): ReLU()\n",
      ")\n",
      "6903\n"
     ]
    }
   ],
   "source": [
    "# Instantiate your model here\n",
    "model1 = Net(D_in, 150, D_out)\n",
    "print(model1)\n",
    "pytorch_total_params = sum(p.numel() for p in model1.parameters() if p.requires_grad)\n",
    "print(pytorch_total_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 finding the best model:\n",
    "Identify, amongst the following options the best parameters for your model:\n",
    "\n",
    "* `criterion` : [CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html), [BCEWithLogitsLoss](hhttps://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html)\n",
    "* `iterations` : 150, 250, 500\n",
    "* `learning rate` : 0.00005, 0.001, 12.031\n",
    "\n",
    "\n",
    "_Hint: restart your runtime between each execution to ensure that previous neural networks dont interfere with your current one_\n",
    "\n",
    "_You can evaluate your model based on it's accuracy over the test set_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Xx9l8UOoefVq"
   },
   "outputs": [],
   "source": [
    "# Define your loss function here:\n",
    "criterion = torch.nn.BCEWithLogitsLoss()#reduction='sum')\n",
    "# Define your Adam optimizer for finding the weights of the network here\n",
    "optimizer = torch.optim.Adam(model1.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z_pmZ0yAefVr",
    "outputId": "f065c474-ad2e-441b-b955-9e3193df2c87"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.6947119832038879\n",
      "1 0.6901298761367798\n",
      "2 0.6856700778007507\n",
      "3 0.6813250780105591\n",
      "4 0.6770883202552795\n",
      "5 0.6729517579078674\n",
      "6 0.6689069867134094\n",
      "7 0.6649466753005981\n",
      "8 0.6610652804374695\n",
      "9 0.6572574973106384\n",
      "10 0.6535206437110901\n",
      "11 0.6498522162437439\n",
      "12 0.6462506055831909\n",
      "13 0.6427140235900879\n",
      "14 0.6392439007759094\n",
      "15 0.635841965675354\n",
      "16 0.6325107216835022\n",
      "17 0.6292549967765808\n",
      "18 0.6260799765586853\n",
      "19 0.6229898929595947\n",
      "20 0.6199890971183777\n",
      "21 0.6170834898948669\n",
      "22 0.6142786741256714\n",
      "23 0.611579179763794\n",
      "24 0.6089863777160645\n",
      "25 0.6065018177032471\n",
      "26 0.6041246056556702\n",
      "27 0.6018514037132263\n",
      "28 0.5996754169464111\n",
      "29 0.5975890755653381\n",
      "30 0.5955836176872253\n",
      "31 0.5936465263366699\n",
      "32 0.591764509677887\n",
      "33 0.5899218916893005\n",
      "34 0.5881047248840332\n",
      "35 0.5862992405891418\n",
      "36 0.5844913721084595\n",
      "37 0.5826683044433594\n",
      "38 0.5808185338973999\n",
      "39 0.5789317488670349\n",
      "40 0.5770004987716675\n",
      "41 0.575019896030426\n",
      "42 0.5729857683181763\n",
      "43 0.5708970427513123\n",
      "44 0.5687533020973206\n",
      "45 0.5665560960769653\n",
      "46 0.564306378364563\n",
      "47 0.5620081424713135\n",
      "48 0.5596641302108765\n",
      "49 0.5572795867919922\n",
      "50 0.5548580288887024\n",
      "51 0.5524039268493652\n",
      "52 0.5499204397201538\n",
      "53 0.5474113821983337\n",
      "54 0.5448799133300781\n",
      "55 0.542328417301178\n",
      "56 0.5397599339485168\n",
      "57 0.5371771454811096\n",
      "58 0.5345809459686279\n",
      "59 0.531972348690033\n",
      "60 0.529352068901062\n",
      "61 0.5267207026481628\n",
      "62 0.524079442024231\n",
      "63 0.5214295983314514\n",
      "64 0.5187718272209167\n",
      "65 0.5161076188087463\n",
      "66 0.5134369134902954\n",
      "67 0.5107614994049072\n",
      "68 0.508082926273346\n",
      "69 0.5054035186767578\n",
      "70 0.5027260184288025\n",
      "71 0.5000520944595337\n",
      "72 0.49738335609436035\n",
      "73 0.4947223663330078\n",
      "74 0.4920709729194641\n",
      "75 0.48943251371383667\n",
      "76 0.4868090748786926\n",
      "77 0.4842012822628021\n",
      "78 0.48161137104034424\n",
      "79 0.4790402352809906\n",
      "80 0.47649064660072327\n",
      "81 0.47396379709243774\n",
      "82 0.47146138548851013\n",
      "83 0.4689842760562897\n",
      "84 0.46653276681900024\n",
      "85 0.46410810947418213\n",
      "86 0.46171262860298157\n",
      "87 0.45934608578681946\n",
      "88 0.45701009035110474\n",
      "89 0.45470550656318665\n",
      "90 0.4524317979812622\n",
      "91 0.4501895606517792\n",
      "92 0.4479779303073883\n",
      "93 0.4457987844944\n",
      "94 0.44365277886390686\n",
      "95 0.44154053926467896\n",
      "96 0.4394630491733551\n",
      "97 0.43741947412490845\n",
      "98 0.43541091680526733\n",
      "99 0.4334377944469452\n",
      "100 0.4315004348754883\n",
      "101 0.429598331451416\n",
      "102 0.4277311861515045\n",
      "103 0.42589911818504333\n",
      "104 0.42410075664520264\n",
      "105 0.4223359525203705\n",
      "106 0.4206047058105469\n",
      "107 0.4189054071903229\n",
      "108 0.41723835468292236\n",
      "109 0.415602445602417\n",
      "110 0.41399747133255005\n",
      "111 0.4124234914779663\n",
      "112 0.4108797311782837\n",
      "113 0.40936514735221863\n",
      "114 0.4078792333602905\n",
      "115 0.4064205586910248\n",
      "116 0.40498852729797363\n",
      "117 0.40358301997184753\n",
      "118 0.40220263600349426\n",
      "119 0.4008469879627228\n",
      "120 0.3995155692100525\n",
      "121 0.398207426071167\n",
      "122 0.39692166447639465\n",
      "123 0.3956577181816101\n",
      "124 0.39441490173339844\n",
      "125 0.39319294691085815\n",
      "126 0.3919909596443176\n",
      "127 0.39080873131752014\n",
      "128 0.3896454870700836\n",
      "129 0.38850077986717224\n",
      "130 0.3873736560344696\n",
      "131 0.3862638771533966\n",
      "132 0.38517069816589355\n",
      "133 0.38409340381622314\n",
      "134 0.3830319941043854\n",
      "135 0.381985604763031\n",
      "136 0.3809536099433899\n",
      "137 0.37993553280830383\n",
      "138 0.3789306581020355\n",
      "139 0.377938449382782\n",
      "140 0.37695884704589844\n",
      "141 0.3759914040565491\n",
      "142 0.37503597140312195\n",
      "143 0.37409213185310364\n",
      "144 0.3731595575809479\n",
      "145 0.37223777174949646\n",
      "146 0.37132593989372253\n",
      "147 0.37042367458343506\n",
      "148 0.3695310056209564\n",
      "149 0.3686482608318329\n",
      "150 0.3677746653556824\n",
      "151 0.36691048741340637\n",
      "152 0.3660562336444855\n",
      "153 0.36521127820014954\n",
      "154 0.3643747866153717\n",
      "155 0.3635464906692505\n",
      "156 0.36272549629211426\n",
      "157 0.36191198229789734\n",
      "158 0.3611060082912445\n",
      "159 0.36030811071395874\n",
      "160 0.3595164716243744\n",
      "161 0.358732134103775\n",
      "162 0.3579542934894562\n",
      "163 0.35718271136283875\n",
      "164 0.3564184308052063\n",
      "165 0.3556618094444275\n",
      "166 0.3549109697341919\n",
      "167 0.3541668951511383\n",
      "168 0.35342878103256226\n",
      "169 0.3526967167854309\n",
      "170 0.3519691228866577\n",
      "171 0.35124653577804565\n",
      "172 0.3505282402038574\n",
      "173 0.3498164117336273\n",
      "174 0.34910848736763\n",
      "175 0.3484051525592804\n",
      "176 0.34770745038986206\n",
      "177 0.34701550006866455\n",
      "178 0.3463287353515625\n",
      "179 0.3456464111804962\n",
      "180 0.34496966004371643\n",
      "181 0.34429705142974854\n",
      "182 0.34363090991973877\n",
      "183 0.3429718017578125\n",
      "184 0.3423185348510742\n",
      "185 0.34167206287384033\n",
      "186 0.34103092551231384\n",
      "187 0.34039580821990967\n",
      "188 0.3397655487060547\n",
      "189 0.33914023637771606\n",
      "190 0.3385199308395386\n",
      "191 0.3379048705101013\n",
      "192 0.33729487657546997\n",
      "193 0.3366895914077759\n",
      "194 0.33608922362327576\n",
      "195 0.3354930877685547\n",
      "196 0.3349009156227112\n",
      "197 0.33431363105773926\n",
      "198 0.3337306082248688\n",
      "199 0.3331517279148102\n",
      "200 0.3325762450695038\n",
      "201 0.33200469613075256\n",
      "202 0.3314371705055237\n",
      "203 0.33087357878685\n",
      "204 0.3303137719631195\n",
      "205 0.3297577500343323\n",
      "206 0.3292054831981659\n",
      "207 0.32865744829177856\n",
      "208 0.3281136453151703\n",
      "209 0.32757386565208435\n",
      "210 0.3270381987094879\n",
      "211 0.3265067934989929\n",
      "212 0.3259795308113098\n",
      "213 0.32545599341392517\n",
      "214 0.3249364495277405\n",
      "215 0.324420303106308\n",
      "216 0.32390713691711426\n",
      "217 0.3233974575996399\n",
      "218 0.3228909969329834\n",
      "219 0.3223879933357239\n",
      "220 0.3218880593776703\n",
      "221 0.32139158248901367\n",
      "222 0.32089856266975403\n",
      "223 0.3204093277454376\n",
      "224 0.31992360949516296\n",
      "225 0.31944143772125244\n",
      "226 0.3189627528190613\n",
      "227 0.3184875249862671\n",
      "228 0.3180159628391266\n",
      "229 0.31754791736602783\n",
      "230 0.3170831501483917\n",
      "231 0.3166216313838959\n",
      "232 0.31616348028182983\n",
      "233 0.31570854783058167\n",
      "234 0.3152570426464081\n",
      "235 0.3148086369037628\n",
      "236 0.3143632709980011\n",
      "237 0.3139212727546692\n",
      "238 0.31348228454589844\n",
      "239 0.3130461573600769\n",
      "240 0.3126131594181061\n",
      "241 0.31218332052230835\n",
      "242 0.3117566406726837\n",
      "243 0.3113330900669098\n",
      "244 0.3109126389026642\n",
      "245 0.3104953467845917\n",
      "246 0.3100810647010803\n",
      "247 0.3096698224544525\n",
      "248 0.3092613220214844\n",
      "249 0.30885574221611023\n",
      "250 0.308452844619751\n",
      "251 0.3080529272556305\n",
      "252 0.307655930519104\n",
      "253 0.30726173520088196\n",
      "254 0.3068700432777405\n",
      "255 0.30648112297058105\n",
      "256 0.30609506368637085\n",
      "257 0.30571186542510986\n",
      "258 0.3053315281867981\n",
      "259 0.3049538731575012\n",
      "260 0.3045789301395416\n",
      "261 0.30420681834220886\n",
      "262 0.30383744835853577\n",
      "263 0.30347102880477905\n",
      "264 0.30310744047164917\n",
      "265 0.3027466833591461\n",
      "266 0.3023885190486908\n",
      "267 0.3020331561565399\n",
      "268 0.3016805946826935\n",
      "269 0.3013305962085724\n",
      "270 0.30098310112953186\n",
      "271 0.3006380498409271\n",
      "272 0.3002956211566925\n",
      "273 0.2999557554721832\n",
      "274 0.2996184229850769\n",
      "275 0.29928356409072876\n",
      "276 0.29895126819610596\n",
      "277 0.29862141609191895\n",
      "278 0.2982938885688782\n",
      "279 0.29796871542930603\n",
      "280 0.2976459562778473\n",
      "281 0.29732534289360046\n",
      "282 0.29700717329978943\n",
      "283 0.29669129848480225\n",
      "284 0.29637768864631653\n",
      "285 0.2960660457611084\n",
      "286 0.2957565486431122\n",
      "287 0.29544928669929504\n",
      "288 0.2951441705226898\n",
      "289 0.2948412299156189\n",
      "290 0.2945403456687927\n",
      "291 0.2942415475845337\n",
      "292 0.29394468665122986\n",
      "293 0.2936498522758484\n",
      "294 0.2933567762374878\n",
      "295 0.29306545853614807\n",
      "296 0.29277580976486206\n",
      "297 0.2924879789352417\n",
      "298 0.2922016382217407\n",
      "299 0.29191702604293823\n",
      "300 0.29163384437561035\n",
      "301 0.29135236144065857\n",
      "302 0.2910725474357605\n",
      "303 0.2907940745353699\n",
      "304 0.2905168831348419\n",
      "305 0.29024142026901245\n",
      "306 0.289967805147171\n",
      "307 0.2896960973739624\n",
      "308 0.28942614793777466\n",
      "309 0.2891573905944824\n",
      "310 0.28889036178588867\n",
      "311 0.28862518072128296\n",
      "312 0.28836187720298767\n",
      "313 0.2880997657775879\n",
      "314 0.2878386974334717\n",
      "315 0.28757861256599426\n",
      "316 0.28731974959373474\n",
      "317 0.28706231713294983\n",
      "318 0.28680628538131714\n",
      "319 0.28655123710632324\n",
      "320 0.28629758954048157\n",
      "321 0.2860451936721802\n",
      "322 0.2857939302921295\n",
      "323 0.2855437397956848\n",
      "324 0.28529471158981323\n",
      "325 0.2850470542907715\n",
      "326 0.28480055928230286\n",
      "327 0.28455501794815063\n",
      "328 0.2843104600906372\n",
      "329 0.2840668261051178\n",
      "330 0.28382408618927\n",
      "331 0.2835823595523834\n",
      "332 0.2833421528339386\n",
      "333 0.28310272097587585\n",
      "334 0.2828638553619385\n",
      "335 0.28262606263160706\n",
      "336 0.282389372587204\n",
      "337 0.2821533977985382\n",
      "338 0.28191834688186646\n",
      "339 0.28168416023254395\n",
      "340 0.28145089745521545\n",
      "341 0.2812185287475586\n",
      "342 0.2809872329235077\n",
      "343 0.28075674176216125\n",
      "344 0.2805272340774536\n",
      "345 0.2802983224391937\n",
      "346 0.2800701856613159\n",
      "347 0.27984291315078735\n",
      "348 0.27961644530296326\n",
      "349 0.27939072251319885\n",
      "350 0.2791655957698822\n",
      "351 0.27894148230552673\n",
      "352 0.2787182331085205\n",
      "353 0.2784956693649292\n",
      "354 0.27827373147010803\n",
      "355 0.27805235981941223\n",
      "356 0.2778316140174866\n",
      "357 0.27761146426200867\n",
      "358 0.27739211916923523\n",
      "359 0.27717354893684387\n",
      "360 0.27695566415786743\n",
      "361 0.27673858404159546\n",
      "362 0.2765221893787384\n",
      "363 0.2763064205646515\n",
      "364 0.27609118819236755\n",
      "365 0.2758767306804657\n",
      "366 0.27566298842430115\n",
      "367 0.2754499912261963\n",
      "368 0.275237500667572\n",
      "369 0.27502551674842834\n",
      "370 0.2748143970966339\n",
      "371 0.27460387349128723\n",
      "372 0.27439403533935547\n",
      "373 0.2741851210594177\n",
      "374 0.2739771902561188\n",
      "375 0.27377012372016907\n",
      "376 0.27356404066085815\n",
      "377 0.27335911989212036\n",
      "378 0.273155152797699\n",
      "379 0.27295199036598206\n",
      "380 0.27274981141090393\n",
      "381 0.27254873514175415\n",
      "382 0.2723485827445984\n",
      "383 0.27214938402175903\n",
      "384 0.271951287984848\n",
      "385 0.27175411581993103\n",
      "386 0.2715579569339752\n",
      "387 0.2713627219200134\n",
      "388 0.27116841077804565\n",
      "389 0.2709749937057495\n",
      "390 0.27078258991241455\n",
      "391 0.27059119939804077\n",
      "392 0.2704005837440491\n",
      "393 0.270210862159729\n",
      "394 0.2700220048427582\n",
      "395 0.2698339521884918\n",
      "396 0.2696467638015747\n",
      "397 0.2694603204727173\n",
      "398 0.2692747116088867\n",
      "399 0.26909005641937256\n",
      "400 0.26890620589256287\n",
      "401 0.26872313022613525\n",
      "402 0.26854071021080017\n",
      "403 0.26835888624191284\n",
      "404 0.26817774772644043\n",
      "405 0.2679973840713501\n",
      "406 0.26781779527664185\n",
      "407 0.2676387429237366\n",
      "408 0.267460435628891\n",
      "409 0.26728302240371704\n",
      "410 0.26710638403892517\n",
      "411 0.2669304311275482\n",
      "412 0.26675522327423096\n",
      "413 0.2665807902812958\n",
      "414 0.2664070725440979\n",
      "415 0.2662338316440582\n",
      "416 0.26606112718582153\n",
      "417 0.2658889889717102\n",
      "418 0.26571744680404663\n",
      "419 0.2655465304851532\n",
      "420 0.2653762698173523\n",
      "421 0.26520660519599915\n",
      "422 0.26503750681877136\n",
      "423 0.26486897468566895\n",
      "424 0.26470088958740234\n",
      "425 0.2645334005355835\n",
      "426 0.26436638832092285\n",
      "427 0.26419976353645325\n",
      "428 0.26403358578681946\n",
      "429 0.2638677954673767\n",
      "430 0.26370248198509216\n",
      "431 0.2635376751422882\n",
      "432 0.26337334513664246\n",
      "433 0.2632094919681549\n",
      "434 0.2630460858345032\n",
      "435 0.2628833055496216\n",
      "436 0.2627210319042206\n",
      "437 0.26255935430526733\n",
      "438 0.2623981535434723\n",
      "439 0.262237548828125\n",
      "440 0.26207754015922546\n",
      "441 0.2619180381298065\n",
      "442 0.2617590129375458\n",
      "443 0.26160046458244324\n",
      "444 0.2614424228668213\n",
      "445 0.26128482818603516\n",
      "446 0.2611277401447296\n",
      "447 0.2609710097312927\n",
      "448 0.2608145475387573\n",
      "449 0.2606583833694458\n",
      "450 0.26050254702568054\n",
      "451 0.26034700870513916\n",
      "452 0.2601916790008545\n",
      "453 0.2600367069244385\n",
      "454 0.25988179445266724\n",
      "455 0.2597270905971527\n",
      "456 0.2595725357532501\n",
      "457 0.2594181299209595\n",
      "458 0.2592640817165375\n",
      "459 0.25911033153533936\n",
      "460 0.2589566707611084\n",
      "461 0.25880318880081177\n",
      "462 0.2586497366428375\n",
      "463 0.2584964334964752\n",
      "464 0.25834324955940247\n",
      "465 0.25819021463394165\n",
      "466 0.25803735852241516\n",
      "467 0.25788453221321106\n",
      "468 0.25773200392723083\n",
      "469 0.25757959485054016\n",
      "470 0.2574276626110077\n",
      "471 0.25727587938308716\n",
      "472 0.2571242153644562\n",
      "473 0.25697261095046997\n",
      "474 0.2568211853504181\n",
      "475 0.2566700577735901\n",
      "476 0.25651898980140686\n",
      "477 0.25636810064315796\n",
      "478 0.25621750950813293\n",
      "479 0.2560670077800751\n",
      "480 0.2559167444705963\n",
      "481 0.2557666301727295\n",
      "482 0.2556167542934418\n",
      "483 0.25546741485595703\n",
      "484 0.2553181052207947\n",
      "485 0.255169153213501\n",
      "486 0.2550206184387207\n",
      "487 0.2548724114894867\n",
      "488 0.25472450256347656\n",
      "489 0.25457701086997986\n",
      "490 0.25442978739738464\n",
      "491 0.25428298115730286\n",
      "492 0.25413641333580017\n",
      "493 0.2539900243282318\n",
      "494 0.2538438141345978\n",
      "495 0.2536979019641876\n",
      "496 0.25355249643325806\n",
      "497 0.25340747833251953\n",
      "498 0.25326287746429443\n",
      "499 0.2531185746192932\n"
     ]
    }
   ],
   "source": [
    "# Perform your iterations here\n",
    "losses1 = []\n",
    "losses1_test = []\n",
    "\n",
    "for t in range(500):                # 500 iterations\n",
    "\n",
    "    # Forward pass: compute prediction on training set\n",
    "    y_pred_train = model1(X_train)\n",
    "    y_pred_test = model1(X_test)\n",
    "    # Compute loss\n",
    "    loss = criterion(y_pred_train, y_train)\n",
    "    print(t, loss.item())\n",
    "    losses1.append(loss.item())\n",
    "    if torch.isnan(loss):\n",
    "        break\n",
    "\n",
    "    # Compute gradient\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    # Update\n",
    "    optimizer.step()\n",
    "\n",
    "    # Compute loss on test set\n",
    "    losses1_test.append(criterion(model1(X_test), y_test).item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Model Accuracy\n",
    "Identify the models accuracy over the train and test parts of the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8309094020018821\n",
      "0.8217279726261763\n"
     ]
    }
   ],
   "source": [
    "# deactivate dropout layers\n",
    "model1.eval()\n",
    "# Training accuracy\n",
    "\n",
    "# Convert y_train to a NumPy array if it's not already\n",
    "y_train = y_train.numpy() if isinstance(y_train, torch.Tensor) else y_train\n",
    "\n",
    "# Make sure both y_pred_train and y_train are 1D arrays of class labels\n",
    "y_pred_train = y_pred_train.argmax(axis=1) if len(y_pred_train.shape) > 1 else y_pred_train\n",
    "y_train = y_train.argmax(axis=1) if len(y_train.shape) > 1 else y_train\n",
    "\n",
    "# Check that the data types are integers\n",
    "#y_pred_train = y_pred_train.astype(int)\n",
    "y_train = y_train.astype(int)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy_train = accuracy_score(y_train, y_pred_train)\n",
    "print(accuracy_train)\n",
    "\n",
    "# test accuracy\n",
    "y_test = y_test.numpy() if isinstance(y_test, torch.Tensor) else y_test\n",
    "\n",
    "# Make sure both y_pred_train and y_train are 1D arrays of class labels\n",
    "y_pred_test = y_pred_test.argmax(axis=1) if len(y_pred_test.shape) > 1 else y_pred_test\n",
    "y_test = y_test.argmax(axis=1) if len(y_test.shape) > 1 else y_test\n",
    "\n",
    "# Check that the data types are integers\n",
    "#y_pred_test = y_pred_test.astype(int)\n",
    "y_test = y_test.astype(int)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(accuracy_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Predictions over the suspects dataset\n",
    "## 3.1 Retrain a new model over the full training dataset\n",
    "#### Please use the following parameters for this section:\n",
    "* ``neurons`` = 150\n",
    "* ``learning`` rate = 0.00005\n",
    "* ``criterion`` = CrossEntropyLoss\n",
    "* `iterations` = 500\n",
    "\n",
    "_hint you may have to redo some preprocessing as you did in part one_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 714 entries, 0 to 1237\n",
      "Data columns (total 42 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   Age                       714 non-null    float64\n",
      " 1   Annual_Income             714 non-null    float64\n",
      " 2   Monthly_Inh_Salary        714 non-null    float64\n",
      " 3   Num_Bank_Accounts         714 non-null    float64\n",
      " 4   Num_Credit_Card           714 non-null    float64\n",
      " 5   Interest_Rate             714 non-null    float64\n",
      " 6   Num_of_Loan               714 non-null    float64\n",
      " 7   Delay_from_due_date       714 non-null    float64\n",
      " 8   Num_of_Delayed_Payment    714 non-null    float64\n",
      " 9   Changed_Credit_Limit      714 non-null    float64\n",
      " 10  Num_Credit_Inquiries      714 non-null    float64\n",
      " 11  Outsting_Debt             714 non-null    float64\n",
      " 12  Credit_Utilization_Ratio  714 non-null    float64\n",
      " 13  Payment_of_Min_Amount     714 non-null    int64  \n",
      " 14  Total_EMI_per_month       714 non-null    float64\n",
      " 15  Amount_invested_monthly   714 non-null    float64\n",
      " 16  Payment_Behaviour         714 non-null    int64  \n",
      " 17  Monthly_Balance           714 non-null    float64\n",
      " 18  AutoLoan                  714 non-null    float64\n",
      " 19  Credit-BuilderLoan        714 non-null    float64\n",
      " 20  DebtConsolidationLoan     714 non-null    float64\n",
      " 21  HomeEquityLoan            714 non-null    float64\n",
      " 22  MortgageLoan              714 non-null    float64\n",
      " 23  NotSpecified              714 non-null    float64\n",
      " 24  PaydayLoan                714 non-null    float64\n",
      " 25  PersonalLoan              714 non-null    float64\n",
      " 26  StudentLoan               714 non-null    float64\n",
      " 27  Occupation_Accountant     714 non-null    float64\n",
      " 28  Occupation_Architect      714 non-null    float64\n",
      " 29  Occupation_Developer      714 non-null    float64\n",
      " 30  Occupation_Doctor         714 non-null    float64\n",
      " 31  Occupation_Engineer       714 non-null    float64\n",
      " 32  Occupation_Entrepreneur   714 non-null    float64\n",
      " 33  Occupation_Journalist     714 non-null    float64\n",
      " 34  Occupation_Lawyer         714 non-null    float64\n",
      " 35  Occupation_Manager        714 non-null    float64\n",
      " 36  Occupation_Mechanic       714 non-null    float64\n",
      " 37  Occupation_MediaManager   714 non-null    float64\n",
      " 38  Occupation_Musician       714 non-null    float64\n",
      " 39  Occupation_Scientist      714 non-null    float64\n",
      " 40  Occupation_Teacher        714 non-null    float64\n",
      " 41  Occupation_Writer         714 non-null    float64\n",
      "dtypes: float64(40), int64(2)\n",
      "memory usage: 239.9 KB\n",
      "None\n",
      "42 3 42\n"
     ]
    }
   ],
   "source": [
    "#preprocessing: X = training and suspects is testing dataset\n",
    "suspects = suspects.drop(columns=['userID'])\n",
    "\n",
    "#y_fitted2 = ohe.fit_transform(y_global[['Credit_Mix']])\n",
    "#categories_credit_mix2 = ohe.get_feature_names_out(input_features=['Credit_Mix'])\n",
    "#y2 = pd.DataFrame(y_fitted2, columns=categories_credit_mix2, index= y_global.index)\n",
    "#print(y2)\n",
    "\n",
    "print(suspects.info())\n",
    "\n",
    "X = torch.tensor(X.values, dtype=torch.float)\n",
    "y = torch.tensor(y.values, dtype=torch.float)\n",
    "\n",
    "#suspects = torch.tensor(suspects.values, dtype=torch.float)\n",
    "\n",
    "print(X.shape[1], y.shape[1], suspects.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, D_in, H1, D_out):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.linear1 = nn.Linear(D_in, H1)        # Linear transformation for hidden layer\n",
    "        self.linear2 = nn.Linear(H1, D_out)       # Linear transformation for output layer\n",
    "        self.activation = nn.ReLU()               # Activation function for hidden layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        y_pred = self.activation(self.linear1(x))   # Hidden layer: linear transformation + ReLU\n",
    "        y_pred = self.linear2(y_pred)               # Output layer: linear transformation\n",
    "        return y_pred\n",
    "\n",
    "# Define the input and output sizes\n",
    "D_in =  X.shape[1]\n",
    "D_out =  y.shape[1]\n",
    "\n",
    "model2 = Net(D_in, 150 , D_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your MSE loss here:\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Define your Adam optimizer for finding the weights of the network here:\n",
    "optimizer = torch.optim.Adam(model1.parameters(), lr=0.00005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.0851341485977173\n",
      "1 1.0851341485977173\n",
      "2 1.0851341485977173\n",
      "3 1.0851341485977173\n",
      "4 1.0851341485977173\n",
      "5 1.0851341485977173\n",
      "6 1.0851341485977173\n",
      "7 1.0851341485977173\n",
      "8 1.0851341485977173\n",
      "9 1.0851341485977173\n",
      "10 1.0851341485977173\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 1.0851341485977173\n",
      "12 1.0851341485977173\n",
      "13 1.0851341485977173\n",
      "14 1.0851341485977173\n",
      "15 1.0851341485977173\n",
      "16 1.0851341485977173\n",
      "17 1.0851341485977173\n",
      "18 1.0851341485977173\n",
      "19 1.0851341485977173\n",
      "20 1.0851341485977173\n",
      "21 1.0851341485977173\n",
      "22 1.0851341485977173\n",
      "23 1.0851341485977173\n",
      "24 1.0851341485977173\n",
      "25 1.0851341485977173\n",
      "26 1.0851341485977173\n",
      "27 1.0851341485977173\n",
      "28 1.0851341485977173\n",
      "29 1.0851341485977173\n",
      "30 1.0851341485977173\n",
      "31 1.0851341485977173\n",
      "32 1.0851341485977173\n",
      "33 1.0851341485977173\n",
      "34 1.0851341485977173\n",
      "35 1.0851341485977173\n",
      "36 1.0851341485977173\n",
      "37 1.0851341485977173\n",
      "38 1.0851341485977173\n",
      "39 1.0851341485977173\n",
      "40 1.0851341485977173\n",
      "41 1.0851341485977173\n",
      "42 1.0851341485977173\n",
      "43 1.0851341485977173\n",
      "44 1.0851341485977173\n",
      "45 1.0851341485977173\n",
      "46 1.0851341485977173\n",
      "47 1.0851341485977173\n",
      "48 1.0851341485977173\n",
      "49 1.0851341485977173\n",
      "50 1.0851341485977173\n",
      "51 1.0851341485977173\n",
      "52 1.0851341485977173\n",
      "53 1.0851341485977173\n",
      "54 1.0851341485977173\n",
      "55 1.0851341485977173\n",
      "56 1.0851341485977173\n",
      "57 1.0851341485977173\n",
      "58 1.0851341485977173\n",
      "59 1.0851341485977173\n",
      "60 1.0851341485977173\n",
      "61 1.0851341485977173\n",
      "62 1.0851341485977173\n",
      "63 1.0851341485977173\n",
      "64 1.0851341485977173\n",
      "65 1.0851341485977173\n",
      "66 1.0851341485977173\n",
      "67 1.0851341485977173\n",
      "68 1.0851341485977173\n",
      "69 1.0851341485977173\n",
      "70 1.0851341485977173\n",
      "71 1.0851341485977173\n",
      "72 1.0851341485977173\n",
      "73 1.0851341485977173\n",
      "74 1.0851341485977173\n",
      "75 1.0851341485977173\n",
      "76 1.0851341485977173\n",
      "77 1.0851341485977173\n",
      "78 1.0851341485977173\n",
      "79 1.0851341485977173\n",
      "80 1.0851341485977173\n",
      "81 1.0851341485977173\n",
      "82 1.0851341485977173\n",
      "83 1.0851341485977173\n",
      "84 1.0851341485977173\n",
      "85 1.0851341485977173\n",
      "86 1.0851341485977173\n",
      "87 1.0851341485977173\n",
      "88 1.0851341485977173\n",
      "89 1.0851341485977173\n",
      "90 1.0851341485977173\n",
      "91 1.0851341485977173\n",
      "92 1.0851341485977173\n",
      "93 1.0851341485977173\n",
      "94 1.0851341485977173\n",
      "95 1.0851341485977173\n",
      "96 1.0851341485977173\n",
      "97 1.0851341485977173\n",
      "98 1.0851341485977173\n",
      "99 1.0851341485977173\n",
      "100 1.0851341485977173\n",
      "101 1.0851341485977173\n",
      "102 1.0851341485977173\n",
      "103 1.0851341485977173\n",
      "104 1.0851341485977173\n",
      "105 1.0851341485977173\n",
      "106 1.0851341485977173\n",
      "107 1.0851341485977173\n",
      "108 1.0851341485977173\n",
      "109 1.0851341485977173\n",
      "110 1.0851341485977173\n",
      "111 1.0851341485977173\n",
      "112 1.0851341485977173\n",
      "113 1.0851341485977173\n",
      "114 1.0851341485977173\n",
      "115 1.0851341485977173\n",
      "116 1.0851341485977173\n",
      "117 1.0851341485977173\n",
      "118 1.0851341485977173\n",
      "119 1.0851341485977173\n",
      "120 1.0851341485977173\n",
      "121 1.0851341485977173\n",
      "122 1.0851341485977173\n",
      "123 1.0851341485977173\n",
      "124 1.0851341485977173\n",
      "125 1.0851341485977173\n",
      "126 1.0851341485977173\n",
      "127 1.0851341485977173\n",
      "128 1.0851341485977173\n",
      "129 1.0851341485977173\n",
      "130 1.0851341485977173\n",
      "131 1.0851341485977173\n",
      "132 1.0851341485977173\n",
      "133 1.0851341485977173\n",
      "134 1.0851341485977173\n",
      "135 1.0851341485977173\n",
      "136 1.0851341485977173\n",
      "137 1.0851341485977173\n",
      "138 1.0851341485977173\n",
      "139 1.0851341485977173\n",
      "140 1.0851341485977173\n",
      "141 1.0851341485977173\n",
      "142 1.0851341485977173\n",
      "143 1.0851341485977173\n",
      "144 1.0851341485977173\n",
      "145 1.0851341485977173\n",
      "146 1.0851341485977173\n",
      "147 1.0851341485977173\n",
      "148 1.0851341485977173\n",
      "149 1.0851341485977173\n",
      "150 1.0851341485977173\n",
      "151 1.0851341485977173\n",
      "152 1.0851341485977173\n",
      "153 1.0851341485977173\n",
      "154 1.0851341485977173\n",
      "155 1.0851341485977173\n",
      "156 1.0851341485977173\n",
      "157 1.0851341485977173\n",
      "158 1.0851341485977173\n",
      "159 1.0851341485977173\n",
      "160 1.0851341485977173\n",
      "161 1.0851341485977173\n",
      "162 1.0851341485977173\n",
      "163 1.0851341485977173\n",
      "164 1.0851341485977173\n",
      "165 1.0851341485977173\n",
      "166 1.0851341485977173\n",
      "167 1.0851341485977173\n",
      "168 1.0851341485977173\n",
      "169 1.0851341485977173\n",
      "170 1.0851341485977173\n",
      "171 1.0851341485977173\n",
      "172 1.0851341485977173\n",
      "173 1.0851341485977173\n",
      "174 1.0851341485977173\n",
      "175 1.0851341485977173\n",
      "176 1.0851341485977173\n",
      "177 1.0851341485977173\n",
      "178 1.0851341485977173\n",
      "179 1.0851341485977173\n",
      "180 1.0851341485977173\n",
      "181 1.0851341485977173\n",
      "182 1.0851341485977173\n",
      "183 1.0851341485977173\n",
      "184 1.0851341485977173\n",
      "185 1.0851341485977173\n",
      "186 1.0851341485977173\n",
      "187 1.0851341485977173\n",
      "188 1.0851341485977173\n",
      "189 1.0851341485977173\n",
      "190 1.0851341485977173\n",
      "191 1.0851341485977173\n",
      "192 1.0851341485977173\n",
      "193 1.0851341485977173\n",
      "194 1.0851341485977173\n",
      "195 1.0851341485977173\n",
      "196 1.0851341485977173\n",
      "197 1.0851341485977173\n",
      "198 1.0851341485977173\n",
      "199 1.0851341485977173\n",
      "200 1.0851341485977173\n",
      "201 1.0851341485977173\n",
      "202 1.0851341485977173\n",
      "203 1.0851341485977173\n",
      "204 1.0851341485977173\n",
      "205 1.0851341485977173\n",
      "206 1.0851341485977173\n",
      "207 1.0851341485977173\n",
      "208 1.0851341485977173\n",
      "209 1.0851341485977173\n",
      "210 1.0851341485977173\n",
      "211 1.0851341485977173\n",
      "212 1.0851341485977173\n",
      "213 1.0851341485977173\n",
      "214 1.0851341485977173\n",
      "215 1.0851341485977173\n",
      "216 1.0851341485977173\n",
      "217 1.0851341485977173\n",
      "218 1.0851341485977173\n",
      "219 1.0851341485977173\n",
      "220 1.0851341485977173\n",
      "221 1.0851341485977173\n",
      "222 1.0851341485977173\n",
      "223 1.0851341485977173\n",
      "224 1.0851341485977173\n",
      "225 1.0851341485977173\n",
      "226 1.0851341485977173\n",
      "227 1.0851341485977173\n",
      "228 1.0851341485977173\n",
      "229 1.0851341485977173\n",
      "230 1.0851341485977173\n",
      "231 1.0851341485977173\n",
      "232 1.0851341485977173\n",
      "233 1.0851341485977173\n",
      "234 1.0851341485977173\n",
      "235 1.0851341485977173\n",
      "236 1.0851341485977173\n",
      "237 1.0851341485977173\n",
      "238 1.0851341485977173\n",
      "239 1.0851341485977173\n",
      "240 1.0851341485977173\n",
      "241 1.0851341485977173\n",
      "242 1.0851341485977173\n",
      "243 1.0851341485977173\n",
      "244 1.0851341485977173\n",
      "245 1.0851341485977173\n",
      "246 1.0851341485977173\n",
      "247 1.0851341485977173\n",
      "248 1.0851341485977173\n",
      "249 1.0851341485977173\n",
      "250 1.0851341485977173\n",
      "251 1.0851341485977173\n",
      "252 1.0851341485977173\n",
      "253 1.0851341485977173\n",
      "254 1.0851341485977173\n",
      "255 1.0851341485977173\n",
      "256 1.0851341485977173\n",
      "257 1.0851341485977173\n",
      "258 1.0851341485977173\n",
      "259 1.0851341485977173\n",
      "260 1.0851341485977173\n",
      "261 1.0851341485977173\n",
      "262 1.0851341485977173\n",
      "263 1.0851341485977173\n",
      "264 1.0851341485977173\n",
      "265 1.0851341485977173\n",
      "266 1.0851341485977173\n",
      "267 1.0851341485977173\n",
      "268 1.0851341485977173\n",
      "269 1.0851341485977173\n",
      "270 1.0851341485977173\n",
      "271 1.0851341485977173\n",
      "272 1.0851341485977173\n",
      "273 1.0851341485977173\n",
      "274 1.0851341485977173\n",
      "275 1.0851341485977173\n",
      "276 1.0851341485977173\n",
      "277 1.0851341485977173\n",
      "278 1.0851341485977173\n",
      "279 1.0851341485977173\n",
      "280 1.0851341485977173\n",
      "281 1.0851341485977173\n",
      "282 1.0851341485977173\n",
      "283 1.0851341485977173\n",
      "284 1.0851341485977173\n",
      "285 1.0851341485977173\n",
      "286 1.0851341485977173\n",
      "287 1.0851341485977173\n",
      "288 1.0851341485977173\n",
      "289 1.0851341485977173\n",
      "290 1.0851341485977173\n",
      "291 1.0851341485977173\n",
      "292 1.0851341485977173\n",
      "293 1.0851341485977173\n",
      "294 1.0851341485977173\n",
      "295 1.0851341485977173\n",
      "296 1.0851341485977173\n",
      "297 1.0851341485977173\n",
      "298 1.0851341485977173\n",
      "299 1.0851341485977173\n",
      "300 1.0851341485977173\n",
      "301 1.0851341485977173\n",
      "302 1.0851341485977173\n",
      "303 1.0851341485977173\n",
      "304 1.0851341485977173\n",
      "305 1.0851341485977173\n",
      "306 1.0851341485977173\n",
      "307 1.0851341485977173\n",
      "308 1.0851341485977173\n",
      "309 1.0851341485977173\n",
      "310 1.0851341485977173\n",
      "311 1.0851341485977173\n",
      "312 1.0851341485977173\n",
      "313 1.0851341485977173\n",
      "314 1.0851341485977173\n",
      "315 1.0851341485977173\n",
      "316 1.0851341485977173\n",
      "317 1.0851341485977173\n",
      "318 1.0851341485977173\n",
      "319 1.0851341485977173\n",
      "320 1.0851341485977173\n",
      "321 1.0851341485977173\n",
      "322 1.0851341485977173\n",
      "323 1.0851341485977173\n",
      "324 1.0851341485977173\n",
      "325 1.0851341485977173\n",
      "326 1.0851341485977173\n",
      "327 1.0851341485977173\n",
      "328 1.0851341485977173\n",
      "329 1.0851341485977173\n",
      "330 1.0851341485977173\n",
      "331 1.0851341485977173\n",
      "332 1.0851341485977173\n",
      "333 1.0851341485977173\n",
      "334 1.0851341485977173\n",
      "335 1.0851341485977173\n",
      "336 1.0851341485977173\n",
      "337 1.0851341485977173\n",
      "338 1.0851341485977173\n",
      "339 1.0851341485977173\n",
      "340 1.0851341485977173\n",
      "341 1.0851341485977173\n",
      "342 1.0851341485977173\n",
      "343 1.0851341485977173\n",
      "344 1.0851341485977173\n",
      "345 1.0851341485977173\n",
      "346 1.0851341485977173\n",
      "347 1.0851341485977173\n",
      "348 1.0851341485977173\n",
      "349 1.0851341485977173\n",
      "350 1.0851341485977173\n",
      "351 1.0851341485977173\n",
      "352 1.0851341485977173\n",
      "353 1.0851341485977173\n",
      "354 1.0851341485977173\n",
      "355 1.0851341485977173\n",
      "356 1.0851341485977173\n",
      "357 1.0851341485977173\n",
      "358 1.0851341485977173\n",
      "359 1.0851341485977173\n",
      "360 1.0851341485977173\n",
      "361 1.0851341485977173\n",
      "362 1.0851341485977173\n",
      "363 1.0851341485977173\n",
      "364 1.0851341485977173\n",
      "365 1.0851341485977173\n",
      "366 1.0851341485977173\n",
      "367 1.0851341485977173\n",
      "368 1.0851341485977173\n",
      "369 1.0851341485977173\n",
      "370 1.0851341485977173\n",
      "371 1.0851341485977173\n",
      "372 1.0851341485977173\n",
      "373 1.0851341485977173\n",
      "374 1.0851341485977173\n",
      "375 1.0851341485977173\n",
      "376 1.0851341485977173\n",
      "377 1.0851341485977173\n",
      "378 1.0851341485977173\n",
      "379 1.0851341485977173\n",
      "380 1.0851341485977173\n",
      "381 1.0851341485977173\n",
      "382 1.0851341485977173\n",
      "383 1.0851341485977173\n",
      "384 1.0851341485977173\n",
      "385 1.0851341485977173\n",
      "386 1.0851341485977173\n",
      "387 1.0851341485977173\n",
      "388 1.0851341485977173\n",
      "389 1.0851341485977173\n",
      "390 1.0851341485977173\n",
      "391 1.0851341485977173\n",
      "392 1.0851341485977173\n",
      "393 1.0851341485977173\n",
      "394 1.0851341485977173\n",
      "395 1.0851341485977173\n",
      "396 1.0851341485977173\n",
      "397 1.0851341485977173\n",
      "398 1.0851341485977173\n",
      "399 1.0851341485977173\n",
      "400 1.0851341485977173\n",
      "401 1.0851341485977173\n",
      "402 1.0851341485977173\n",
      "403 1.0851341485977173\n",
      "404 1.0851341485977173\n",
      "405 1.0851341485977173\n",
      "406 1.0851341485977173\n",
      "407 1.0851341485977173\n",
      "408 1.0851341485977173\n",
      "409 1.0851341485977173\n",
      "410 1.0851341485977173\n",
      "411 1.0851341485977173\n",
      "412 1.0851341485977173\n",
      "413 1.0851341485977173\n",
      "414 1.0851341485977173\n",
      "415 1.0851341485977173\n",
      "416 1.0851341485977173\n",
      "417 1.0851341485977173\n",
      "418 1.0851341485977173\n",
      "419 1.0851341485977173\n",
      "420 1.0851341485977173\n",
      "421 1.0851341485977173\n",
      "422 1.0851341485977173\n",
      "423 1.0851341485977173\n",
      "424 1.0851341485977173\n",
      "425 1.0851341485977173\n",
      "426 1.0851341485977173\n",
      "427 1.0851341485977173\n",
      "428 1.0851341485977173\n",
      "429 1.0851341485977173\n",
      "430 1.0851341485977173\n",
      "431 1.0851341485977173\n",
      "432 1.0851341485977173\n",
      "433 1.0851341485977173\n",
      "434 1.0851341485977173\n",
      "435 1.0851341485977173\n",
      "436 1.0851341485977173\n",
      "437 1.0851341485977173\n",
      "438 1.0851341485977173\n",
      "439 1.0851341485977173\n",
      "440 1.0851341485977173\n",
      "441 1.0851341485977173\n",
      "442 1.0851341485977173\n",
      "443 1.0851341485977173\n",
      "444 1.0851341485977173\n",
      "445 1.0851341485977173\n",
      "446 1.0851341485977173\n",
      "447 1.0851341485977173\n",
      "448 1.0851341485977173\n",
      "449 1.0851341485977173\n",
      "450 1.0851341485977173\n",
      "451 1.0851341485977173\n",
      "452 1.0851341485977173\n",
      "453 1.0851341485977173\n",
      "454 1.0851341485977173\n",
      "455 1.0851341485977173\n",
      "456 1.0851341485977173\n",
      "457 1.0851341485977173\n",
      "458 1.0851341485977173\n",
      "459 1.0851341485977173\n",
      "460 1.0851341485977173\n",
      "461 1.0851341485977173\n",
      "462 1.0851341485977173\n",
      "463 1.0851341485977173\n",
      "464 1.0851341485977173\n",
      "465 1.0851341485977173\n",
      "466 1.0851341485977173\n",
      "467 1.0851341485977173\n",
      "468 1.0851341485977173\n",
      "469 1.0851341485977173\n",
      "470 1.0851341485977173\n",
      "471 1.0851341485977173\n",
      "472 1.0851341485977173\n",
      "473 1.0851341485977173\n",
      "474 1.0851341485977173\n",
      "475 1.0851341485977173\n",
      "476 1.0851341485977173\n",
      "477 1.0851341485977173\n",
      "478 1.0851341485977173\n",
      "479 1.0851341485977173\n",
      "480 1.0851341485977173\n",
      "481 1.0851341485977173\n",
      "482 1.0851341485977173\n",
      "483 1.0851341485977173\n",
      "484 1.0851341485977173\n",
      "485 1.0851341485977173\n",
      "486 1.0851341485977173\n",
      "487 1.0851341485977173\n",
      "488 1.0851341485977173\n",
      "489 1.0851341485977173\n",
      "490 1.0851341485977173\n",
      "491 1.0851341485977173\n",
      "492 1.0851341485977173\n",
      "493 1.0851341485977173\n",
      "494 1.0851341485977173\n",
      "495 1.0851341485977173\n",
      "496 1.0851341485977173\n",
      "497 1.0851341485977173\n",
      "498 1.0851341485977173\n",
      "499 1.0851341485977173\n"
     ]
    }
   ],
   "source": [
    "# perform your training here\n",
    "losses1 = []\n",
    "losses1_test = []\n",
    "\n",
    "for t in range(500):                # 500 iterations\n",
    "\n",
    "    # Forward pass: compute prediction on training set\n",
    "    y_pred = model2(X)\n",
    "    #batch_size = min(y_pred.size(0), suspects.size(0))\n",
    "    #y_pred = y_pred[:batch_size]\n",
    "    #suspects = suspects[:batch_size]\n",
    "\n",
    "\n",
    "    # Compute loss\n",
    "    loss = criterion(y_pred, y)\n",
    "    print(t, loss.item())\n",
    "    losses1.append(loss.item())\n",
    "    if torch.isnan(loss):\n",
    "        break\n",
    "\n",
    "    # Compute gradient\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    # Update\n",
    "    optimizer.step()\n",
    "\n",
    "losses1_test.append(criterion(model1(X), y).item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Predict over the suspects dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 2, 2, 0, 0, 0, 2, 2, 2, 2, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 0, 0, 1, 0, 0, 2, 0, 2, 2, 2, 2, 2, 0,\n",
      "        0, 0, 0, 0, 0, 2, 2, 2, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 2, 2, 2, 2, 2, 0, 0, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 2,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 0, 0, 2, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2,\n",
      "        0, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 0, 0, 2, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 2, 0, 0, 0, 0, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 0, 0, 0,\n",
      "        0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 2, 0, 0, 0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2,\n",
      "        2, 0, 0, 0, 0, 0, 0, 2, 2, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        2, 2, 2, 2, 2, 2, 0, 0, 0, 2, 1, 2, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 2, 0,\n",
      "        2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 1, 0, 2, 2, 2, 0, 0, 0, 0, 2, 0, 0, 0,\n",
      "        2, 0, 0, 0, 0, 0, 0, 2, 2, 0, 2, 2, 2, 0, 0, 2, 2, 0, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 0, 2, 2, 2, 0, 0, 0, 2, 2, 2, 0, 0, 0, 0, 0, 2, 0, 2, 0, 0, 2, 2,\n",
      "        2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 0, 0, 2, 2, 2, 2, 0, 0, 0, 2,\n",
      "        0, 0, 0, 0, 1, 0, 0, 0, 0, 2, 0, 1, 2, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 2,\n",
      "        2, 0, 0, 0, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 2, 2, 2, 1, 0, 0,\n",
      "        0, 2, 2, 2, 0, 0, 0, 0, 0, 2, 0, 2, 0, 0, 0, 0, 2, 2, 2, 2, 0, 2, 2, 2,\n",
      "        0, 0, 0, 0, 0, 2, 0, 0, 0, 2, 0, 2, 2, 2, 0, 2, 0, 0, 0, 0, 0, 2, 2, 2,\n",
      "        2, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2,\n",
      "        2, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 0, 2, 2, 2, 0, 0, 0, 0, 2, 0, 0,\n",
      "        2, 1, 0, 0, 0, 0, 2, 2, 2, 0, 0, 0, 0, 0, 2, 0, 2, 0, 0, 0, 0, 2, 2, 2,\n",
      "        0, 0, 2, 2, 0, 0, 0, 2, 2, 2, 1, 0, 0, 0, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0,\n",
      "        2, 2, 2, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0,\n",
      "        2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0, 2, 2, 2, 2])\n",
      "     0\n",
      "0    0\n",
      "1    0\n",
      "2    2\n",
      "3    0\n",
      "4    0\n",
      "..  ..\n",
      "709  0\n",
      "710  2\n",
      "711  2\n",
      "712  2\n",
      "713  2\n",
      "\n",
      "[714 rows x 1 columns]\n",
      "[[False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [ True]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [ True]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [ True]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [ True]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [ True]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [ True]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [ True]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [ True]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [ True]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [ True]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [ True]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [ True]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]]\n",
      "Row number where '1' appears: 61\n",
      "Row number where '1' appears: 243\n",
      "Row number where '1' appears: 256\n",
      "Row number where '1' appears: 322\n",
      "Row number where '1' appears: 347\n",
      "Row number where '1' appears: 436\n",
      "Row number where '1' appears: 443\n",
      "Row number where '1' appears: 477\n",
      "Row number where '1' appears: 530\n",
      "Row number where '1' appears: 601\n",
      "Row number where '1' appears: 634\n",
      "Row number where '1' appears: 688\n"
     ]
    }
   ],
   "source": [
    "# Predict which users have a good credit score here:\n",
    "model2.eval()\n",
    "model2.train()\n",
    "suspenseTensor = torch.tensor(suspects.values, dtype = torch.float)\n",
    "model2(suspenseTensor)\n",
    "print(model2(suspenseTensor).argmax(axis = 1))\n",
    "real_suspects = pd.DataFrame(model2(suspenseTensor).argmax(axis = 1))\n",
    "print(real_suspects)\n",
    "print((real_suspects == 1).values)\n",
    "\n",
    "row_indices, col_indices = np.where(real_suspects == 1)\n",
    "\n",
    "# Print the row numbers where '1' appears\n",
    "for row_index in row_indices:\n",
    "    print(f\"Row number where '1' appears: {row_index}\")\n",
    "\n",
    "#print row x of "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Age                             44.000000\n",
       "Annual_Income                14129.760000\n",
       "Monthly_Inh_Salary            1023.217588\n",
       "Num_Bank_Accounts                8.000000\n",
       "Num_Credit_Card                  5.000000\n",
       "Interest_Rate                   21.000000\n",
       "Num_of_Loan                      8.000000\n",
       "Delay_from_due_date             62.000000\n",
       "Num_of_Delayed_Payment          18.000000\n",
       "Changed_Credit_Limit             5.510000\n",
       "Num_Credit_Inquiries             6.000000\n",
       "Outsting_Debt                 4071.620000\n",
       "Credit_Utilization_Ratio        30.476620\n",
       "Payment_of_Min_Amount            2.000000\n",
       "Total_EMI_per_month             70.975067\n",
       "Amount_invested_monthly         14.896878\n",
       "Payment_Behaviour                2.000000\n",
       "Monthly_Balance                254.638467\n",
       "AutoLoan                         0.000000\n",
       "Credit-BuilderLoan               0.000000\n",
       "DebtConsolidationLoan            2.000000\n",
       "HomeEquityLoan                   0.000000\n",
       "MortgageLoan                     1.000000\n",
       "NotSpecified                     1.000000\n",
       "PaydayLoan                       1.000000\n",
       "PersonalLoan                     1.000000\n",
       "StudentLoan                      2.000000\n",
       "Occupation_Accountant            0.000000\n",
       "Occupation_Architect             0.000000\n",
       "Occupation_Developer             0.000000\n",
       "Occupation_Doctor                0.000000\n",
       "Occupation_Engineer              0.000000\n",
       "Occupation_Entrepreneur          0.000000\n",
       "Occupation_Journalist            0.000000\n",
       "Occupation_Lawyer                0.000000\n",
       "Occupation_Manager               0.000000\n",
       "Occupation_Mechanic              1.000000\n",
       "Occupation_MediaManager          0.000000\n",
       "Occupation_Musician              0.000000\n",
       "Occupation_Scientist             0.000000\n",
       "Occupation_Teacher               0.000000\n",
       "Occupation_Writer                0.000000\n",
       "userID                      539227.000000\n",
       "Name: 209, dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "suspects = pd.read_csv(\"https://raw.githubusercontent.com/michalis0/DataScience_and_MachineLearning/master/Assignements/Part%204/data/suspects.csv\", index_col='Unnamed: 0').dropna()\n",
    "row_index = 209\n",
    "\n",
    "# Use .loc to access the specific row and print the \"userid\"\n",
    "suspects.loc[row_index]"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
