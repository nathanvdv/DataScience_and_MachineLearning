{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a href=\"https://colab.research.google.com/github/michalis0/DataScience_and_MachineLearning/blob/master/Assignements/Part%205/Assignment_part_five.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZu-7QbP9muh"
      },
      "source": [
        "DSML investigation:\n",
        "\n",
        "You are part of the Suisse Impossible Mission Force, or SIMF for short. You need to uncover a rogue agent that is trying to steal sensitive information.\n",
        "\n",
        "Your mission, should you choose to accept it, is to find that agent before stealing any classified information. Good luck!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DyL7WNdV9sWV"
      },
      "source": [
        "# Assignement part five\n",
        "\n",
        "### Due 29.10 (You get an extra hour!)\n",
        "\n",
        "By now you should have 4 suspects left.\n",
        "More information came in that suggests that the rogue agent is tampering with the sentiment annotation system of the SIMF which analyses news documents and marks their sentiment of intelligence analysis tasks.\n",
        "\n",
        "This annotation is crutial to identify documents expressing negativity towards Switzerland and its allies.\n",
        "\n",
        "Each document contains a column which shows which user accessed it. We know that the rogue agent accessed only the documents whose negative sentiment was high, and was then changed to positive or neutral. We will use a huggingface model to identify what records have been tampered with.\n",
        "\n",
        "\n",
        "[You can find more models on this link](https://huggingface.co/models?sort=trending)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "XHhI95r5-tyD",
        "outputId": "516818b8-83db-4be2-a607-5e47278fa763"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install datasets transformers huggingface_hub\n",
        "!apt-get install git-lfs\n",
        "!pip install transformers[torch]\n",
        "!pip install accelerate -U\n",
        "# Import required packages\n",
        "\n",
        "from transformers import pipeline, DataCollatorWithPadding\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "torch.cuda.is_available()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zx8xYrO4CxL-"
      },
      "source": [
        "# 1. Getting to know our data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "RGE_4cFOqxbg"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_excel('https://raw.githubusercontent.com/michalis0/DataScience_and_MachineLearning/master/Assignements/Part%205/data/Reduced_Set_2100.xlsx')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "id": "Fb3Onvokqxbh",
        "outputId": "7aefeed1-a714-47ab-fa42-7cc7e51ecee4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 2100 entries, 0 to 2099\n",
            "Data columns (total 7 columns):\n",
            " #   Column      Non-Null Count  Dtype \n",
            "---  ------      --------------  ----- \n",
            " 0   company     2100 non-null   object\n",
            " 1   title       2100 non-null   object\n",
            " 2   news        2100 non-null   object\n",
            " 3   evaluation  2100 non-null   object\n",
            " 4   year        2100 non-null   int64 \n",
            " 5   month       2100 non-null   int64 \n",
            " 6   day         2087 non-null   object\n",
            "dtypes: int64(2), object(5)\n",
            "memory usage: 115.0+ KB\n"
          ]
        }
      ],
      "source": [
        "df.info(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmH1uBydDB9b"
      },
      "source": [
        "# 2. Re-evaluating with SIMF's model:\n",
        "Evaluate the sentiment on the title column using a sentiment pipeline trained on the `finiteautomata/bertweet-base-sentiment-analysis`model\n",
        "\n",
        "\n",
        "_This may take a while_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "N3TGz8z4Dfh9"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "# Your code here\n",
        "sentiment_pipeline_simf = pipeline(\"sentiment-analysis\", model=\"finiteautomata/bertweet-base-sentiment-analysis\")\n",
        "titles = df.iloc[:, 1]\n",
        "#intitialize\n",
        "sentiments_simf = []\n",
        "confidence_scores_simf = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "XScEdGvVqxbi"
      },
      "outputs": [],
      "source": [
        "# Your code here\n",
        "# Evaluate sentiment for each title\n",
        "for title in titles:\n",
        "    result = sentiment_pipeline_simf(title)\n",
        "    \n",
        "    # Assuming the model returns a single sentiment result\n",
        "    sentiment = result[0]['label']\n",
        "    confidence = result[0]['score']\n",
        "    \n",
        "    sentiments_simf.append(sentiment)\n",
        "    confidence_scores_simf.append(confidence)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "If using all scalar values, you must pass an index",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[1;32m/home/nathan/OneDrive/GitHub/DataScience_and_MachineLearning/Assignements/Part 5/Assignment_part_five.ipynb Cell 11\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/nathan/OneDrive/GitHub/DataScience_and_MachineLearning/Assignements/Part%205/Assignment_part_five.ipynb#X13sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/nathan/OneDrive/GitHub/DataScience_and_MachineLearning/Assignements/Part%205/Assignment_part_five.ipynb#X13sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m simf \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mDataFrame({\u001b[39m\"\u001b[39;49m\u001b[39mSentiment\u001b[39;49m\u001b[39m\"\u001b[39;49m:df})\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/nathan/OneDrive/GitHub/DataScience_and_MachineLearning/Assignements/Part%205/Assignment_part_five.ipynb#X13sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m value_mapping \u001b[39m=\u001b[39m {\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/nathan/OneDrive/GitHub/DataScience_and_MachineLearning/Assignements/Part%205/Assignment_part_five.ipynb#X13sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mneutral\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mNEUTRAL\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/nathan/OneDrive/GitHub/DataScience_and_MachineLearning/Assignements/Part%205/Assignment_part_five.ipynb#X13sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mneutral\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mNEGATIVE\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/nathan/OneDrive/GitHub/DataScience_and_MachineLearning/Assignements/Part%205/Assignment_part_five.ipynb#X13sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mneutral\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mPOSITIVE\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/nathan/OneDrive/GitHub/DataScience_and_MachineLearning/Assignements/Part%205/Assignment_part_five.ipynb#X13sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m }\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/nathan/OneDrive/GitHub/DataScience_and_MachineLearning/Assignements/Part%205/Assignment_part_five.ipynb#X13sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m#simf['Sentiment'] = simf['Sentiment'].replace(value_mapping)\u001b[39;00m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/frame.py:736\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    730\u001b[0m     mgr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_mgr(\n\u001b[1;32m    731\u001b[0m         data, axes\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mindex\u001b[39m\u001b[39m\"\u001b[39m: index, \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m: columns}, dtype\u001b[39m=\u001b[39mdtype, copy\u001b[39m=\u001b[39mcopy\n\u001b[1;32m    732\u001b[0m     )\n\u001b[1;32m    734\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, \u001b[39mdict\u001b[39m):\n\u001b[1;32m    735\u001b[0m     \u001b[39m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[0;32m--> 736\u001b[0m     mgr \u001b[39m=\u001b[39m dict_to_mgr(data, index, columns, dtype\u001b[39m=\u001b[39;49mdtype, copy\u001b[39m=\u001b[39;49mcopy, typ\u001b[39m=\u001b[39;49mmanager)\n\u001b[1;32m    737\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, ma\u001b[39m.\u001b[39mMaskedArray):\n\u001b[1;32m    738\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mnumpy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mma\u001b[39;00m \u001b[39mimport\u001b[39;00m mrecords\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/internals/construction.py:503\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[0;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[1;32m    499\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    500\u001b[0m         \u001b[39m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[1;32m    501\u001b[0m         arrays \u001b[39m=\u001b[39m [x\u001b[39m.\u001b[39mcopy() \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(x, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39melse\u001b[39;00m x \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m arrays]\n\u001b[0;32m--> 503\u001b[0m \u001b[39mreturn\u001b[39;00m arrays_to_mgr(arrays, columns, index, dtype\u001b[39m=\u001b[39;49mdtype, typ\u001b[39m=\u001b[39;49mtyp, consolidate\u001b[39m=\u001b[39;49mcopy)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/internals/construction.py:114\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[39mif\u001b[39;00m verify_integrity:\n\u001b[1;32m    112\u001b[0m     \u001b[39m# figure out the index, if necessary\u001b[39;00m\n\u001b[1;32m    113\u001b[0m     \u001b[39mif\u001b[39;00m index \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 114\u001b[0m         index \u001b[39m=\u001b[39m _extract_index(arrays)\n\u001b[1;32m    115\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    116\u001b[0m         index \u001b[39m=\u001b[39m ensure_index(index)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/internals/construction.py:667\u001b[0m, in \u001b[0;36m_extract_index\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    664\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mPer-column arrays must each be 1-dimensional\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    666\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m indexes \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m raw_lengths:\n\u001b[0;32m--> 667\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mIf using all scalar values, you must pass an index\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    669\u001b[0m \u001b[39mif\u001b[39;00m have_series:\n\u001b[1;32m    670\u001b[0m     index \u001b[39m=\u001b[39m union_indexes(indexes)\n",
            "\u001b[0;31mValueError\u001b[0m: If using all scalar values, you must pass an index"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "simf = pd.DataFrame({\"Sentiment\":sentiments_simf, \"Score\": confidence_scores_simf})\n",
        "value_mapping = {\n",
        "    'NEU': 'NEUTRAL',\n",
        "    'NEG': 'NEGATIVE',\n",
        "    'POS': 'POSITIVE'\n",
        "}\n",
        "simf['Sentiment'] = simf['Sentiment'].replace(value_mapping)\n",
        "\n",
        "print(simf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_RlHub7zdml"
      },
      "source": [
        "## 2.1 How many of the total entries match both the SIMF model **and** the hugginface model?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "WiNq8Ao-zdUh"
      },
      "outputs": [],
      "source": [
        "# Your code here\n",
        "sentiment_pipeline_huggingface = pipeline(\"sentiment-analysis\")\n",
        "sentiment_huggingface = []\n",
        "confidence_scores_huggingface = []\n",
        "for title in titles:\n",
        "    result = sentiment_pipeline_huggingface(title)\n",
        "    \n",
        "    # Assuming the model returns a single sentiment result\n",
        "    sentiment = result[0]['label']\n",
        "    confidence = result[0]['score']\n",
        "    \n",
        "    sentiment_huggingface.append(sentiment)\n",
        "    confidence_scores_huggingface.append(confidence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "     Sentiment     Score\n",
            "0     NEGATIVE  0.937911\n",
            "1     POSITIVE  0.933045\n",
            "2     NEGATIVE  0.999167\n",
            "3     POSITIVE  0.995083\n",
            "4     POSITIVE  0.994639\n",
            "...        ...       ...\n",
            "2095  NEGATIVE  0.998440\n",
            "2096  NEGATIVE  0.992078\n",
            "2097  POSITIVE  0.984533\n",
            "2098  NEGATIVE  0.778633\n",
            "2099  POSITIVE  0.999649\n",
            "\n",
            "[2100 rows x 2 columns]\n"
          ]
        }
      ],
      "source": [
        "huggingface = pd.DataFrame({\"Sentiment\":sentiment_huggingface, \"Score\": confidence_scores_huggingface})\n",
        "print(huggingface)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1400\n"
          ]
        }
      ],
      "source": [
        "df['evaluation'] = df['evaluation'].str.upper()\n",
        "matching_rows_count = len(df[df['evaluation'].isin(huggingface['Sentiment'])])\n",
        "print(matching_rows_count)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7nH3eY5y82X"
      },
      "source": [
        "## 2.2 We will now focus on the entries that do not match\n",
        "#### Identify all non matching entries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "WZfbRN8Y-O4V"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Non-matching entries:\n",
            "     Sentiment     Score\n",
            "0      NEUTRAL  0.883557\n",
            "1      NEUTRAL  0.966678\n",
            "4      NEUTRAL  0.566068\n",
            "5      NEUTRAL  0.554748\n",
            "6      NEUTRAL  0.958037\n",
            "...        ...       ...\n",
            "2092   NEUTRAL  0.819136\n",
            "2093   NEUTRAL  0.527811\n",
            "2095   NEUTRAL  0.740490\n",
            "2096   NEUTRAL  0.939423\n",
            "2098  POSITIVE  0.542725\n",
            "\n",
            "[1444 rows x 2 columns]\n"
          ]
        }
      ],
      "source": [
        "# Your code here\n",
        "non_matching_entries = simf[~simf['Sentiment'].eq(huggingface['Sentiment'])]\n",
        "# Print the non-matching entries\n",
        "print(\"Non-matching entries:\")\n",
        "print(non_matching_entries)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-XZXkjm1uzk"
      },
      "source": [
        "## 2.3 How many of those entries that our model predicted as negative, are evaluate as neutral or positive by the SIMF model ?\n",
        "\n",
        "Store the resulting dataframe into a new one that we will be using in the following questions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "lgZtQbdl19Ek"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "               company                                              title  \\\n",
            "2049           PEPSICO     BRONX ZOO FLIPPING OFF LIGHTS FOR HOLIDAY FEST   \n",
            "2050   WAL MART STORES                             Fish dishes go further   \n",
            "2051           PEPSICO           American giant tries to halve footprints   \n",
            "2053  GENERAL ELECTRIC  Climate group seeks legal action over hoax web...   \n",
            "2058  GENERAL ELECTRIC  Rogers defends climate stance as group challen...   \n",
            "...                ...                                                ...   \n",
            "2034   MERCK & COMPANY  TRENDS   RAIN-FOREST CHIC   Maybe Ben & Jerry'...   \n",
            "2037   WAL MART STORES              Vancouver snub 'disappoints' Wal-Mart   \n",
            "2041   WAL MART STORES  If you can't beat 'em...; Environmentalist Ada...   \n",
            "2043   WAL MART STORES            Science needs to spark a bumper harvest   \n",
            "2046  GENERAL ELECTRIC               Chief Turns to Webcast to Pitch G.E.   \n",
            "\n",
            "                                                   news evaluation  year  \\\n",
            "2049  THE BRONX ZOO is pulling the plug on its annua...   POSITIVE  2008   \n",
            "2050  Australians eat about 20kg of seafood each a y...   POSITIVE  2007   \n",
            "2051  PEPSICO:FOOD and drinks conglomerate PEPSICO i...   POSITIVE  2010   \n",
            "2053  The US Climate Action Partnership, a broad coa...   POSITIVE  2007   \n",
            "2058  Duke Energy CEO Jim Rogers on Thursday rebutte...   POSITIVE  2010   \n",
            "...                                                 ...        ...   ...   \n",
            "2034  […] Roddick told the Mexican conclave of big-b...   POSITIVE  1995   \n",
            "2037  VANCOUVER - A day after Vancouver City Council...   POSITIVE  2005   \n",
            "2041  Not so long ago, the name Adam Werbach carried...   POSITIVE  2008   \n",
            "2043  Innovation in agriculture needs to speed up dr...   POSITIVE  2012   \n",
            "2046  Jeffrey R. Immelt, the chief executive of GENE...   POSITIVE  2008   \n",
            "\n",
            "      month day  \n",
            "2049     11  30  \n",
            "2050      3   7  \n",
            "2051     11  26  \n",
            "2053     12   6  \n",
            "2058      5  10  \n",
            "...     ...  ..  \n",
            "2034      9  29  \n",
            "2037      6  30  \n",
            "2041      3  15  \n",
            "2043      6  25  \n",
            "2046      3  14  \n",
            "\n",
            "[652 rows x 7 columns]\n",
            "              company                                              title  \\\n",
            "0              APPLE   Tourists snap up British iPads to smuggle into...   \n",
            "1             CHEVRON           AFTER SEATTLE; Anarchists get organized.   \n",
            "2         Exxon Mobil                            $10bn oil payout voided   \n",
            "3     WAL MART STORES  Craft capitalism: Just do it yourself; Web mar...   \n",
            "4         Exxon Mobil         Chevron gas project gets state green light   \n",
            "..                ...                                                ...   \n",
            "695       Exxon Mobil  International: ExxonMobil is still funding gro...   \n",
            "696  PROCTER & GAMBLE  Firm says cause of incident was not its respon...   \n",
            "697           CHEVRON                           Nigeria;A Festering Sore   \n",
            "698       Exxon Mobil  Fujian villagers suffer myriad pollution probl...   \n",
            "699           CHEVRON                     CHEVRON confirms Gorgon delays   \n",
            "\n",
            "                                                  news evaluation  year  \\\n",
            "0    IT'S the digital version of the slow boat to C...   NEGATIVE  2011   \n",
            "1    For Juliette Beck, it began with the story of ...   NEGATIVE  2000   \n",
            "2    SAN FRANCISCO: An appeal court yesterday voide...   NEGATIVE  2001   \n",
            "3    The declaration from the Handmade Consortium m...   NEGATIVE  2007   \n",
            "4    SYDNEY: Chevron has received final environment...   NEGATIVE  2007   \n",
            "..                                                 ...        ...   ...   \n",
            "695  The world's largest oil company is continuing ...   NEGATIVE  2009   \n",
            "696  PROCTER & GAMBLE said that a foul drain within...   NEGATIVE  1996   \n",
            "697  The Mobil Idaho spillage which occurred when a...   NEGATIVE  2001   \n",
            "698  Had there not been dozens of petrochemical pla...   NEGATIVE  2009   \n",
            "699  CHEVRON, the leader of the Gorgon LNG joint ve...   NEGATIVE  2006   \n",
            "\n",
            "     month day  \n",
            "0        4  17  \n",
            "1        4  17  \n",
            "2       11   9  \n",
            "3       12  15  \n",
            "4        9   8  \n",
            "..     ...  ..  \n",
            "695      7   2  \n",
            "696      9  14  \n",
            "697      2  26  \n",
            "698      9  14  \n",
            "699      5  20  \n",
            "\n",
            "[700 rows x 7 columns]\n"
          ]
        }
      ],
      "source": [
        "# Your code here\n",
        "negative_huggingface = (huggingface.index[huggingface['Sentiment'] == 'NEGATIVE']).tolist()\n",
        "neu_pos_simf = df[df['evaluation'].isin(['POSITIVE', 'NEUTRAL'])].index.tolist()\n",
        "common_row_indices = list(set(negative_huggingface) & set(neu_pos_simf))\n",
        "\n",
        "# Tampered data\n",
        "tampered_df = df.iloc[common_row_indices]\n",
        "print(tampered_df)\n",
        "\n",
        "condition = ~df['evaluation'].isin(tampered_df['evaluation'])\n",
        "non_tampered_df = df[condition]\n",
        "print(non_tampered_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKzgCDAq7_6O"
      },
      "source": [
        "# 3. Use the ChangeLog dataframe to identify the usersID's who edited the tampered entries, and only the altered entries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "FjA97Cfv9g_j"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 4169 entries, 0 to 4168\n",
            "Data columns (total 2 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   UserID  4169 non-null   object\n",
            " 1   title   4169 non-null   object\n",
            "dtypes: object(2)\n",
            "memory usage: 65.3+ KB\n"
          ]
        }
      ],
      "source": [
        "ChangeLog = pd.read_csv('https://raw.githubusercontent.com/michalis0/DataScience_and_MachineLearning/master/Assignements/Part%205/data/ChangeLog.csv')\n",
        "ChangeLog.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6uH3PNtJ9WHE"
      },
      "source": [
        "## 3.1 Identifying the users who have edited tampered documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "s8j0Rxhx9f8X"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "        UserID                                              title  \\\n",
            "0     [140991]  TRENDS   RAIN-FOREST CHIC   Maybe Ben & Jerry'...   \n",
            "1     [628864]  TRENDS   RAIN-FOREST CHIC   Maybe Ben & Jerry'...   \n",
            "2     [711683]  TRENDS   RAIN-FOREST CHIC   Maybe Ben & Jerry'...   \n",
            "3     [398434]  TRENDS   RAIN-FOREST CHIC   Maybe Ben & Jerry'...   \n",
            "4     [726073]  Food Corporate Giants Should Reverse the Bitte...   \n",
            "...        ...                                                ...   \n",
            "1322  [316450]  No twig left behind; Old into new. Wood waste ...   \n",
            "1323  [255073]  No twig left behind; Old into new. Wood waste ...   \n",
            "1324  [638950]  No twig left behind; Old into new. Wood waste ...   \n",
            "1325  [829946]  GE takes Leap 56 to CF34 successor INTRODUCTIO...   \n",
            "1326  [340001]  GE takes Leap 56 to CF34 successor INTRODUCTIO...   \n",
            "\n",
            "               company                                               news  \\\n",
            "0      MERCK & COMPANY  […] Roddick told the Mexican conclave of big-b...   \n",
            "1      MERCK & COMPANY  […] Roddick told the Mexican conclave of big-b...   \n",
            "2      MERCK & COMPANY  […] Roddick told the Mexican conclave of big-b...   \n",
            "3      MERCK & COMPANY  […] Roddick told the Mexican conclave of big-b...   \n",
            "4              PEPSICO  […] In particular, Oxfam wants these global co...   \n",
            "...                ...                                                ...   \n",
            "1322           CHEVRON  The future of Canada's forests may lie in appl...   \n",
            "1323           CHEVRON  The future of Canada's forests may lie in appl...   \n",
            "1324           CHEVRON  The future of Canada's forests may lie in appl...   \n",
            "1325  GENERAL ELECTRIC  GENERAL ELECTRIC believes a new centreline suc...   \n",
            "1326  GENERAL ELECTRIC  GENERAL ELECTRIC believes a new centreline suc...   \n",
            "\n",
            "     evaluation  year  month day  \n",
            "0      POSITIVE  1995      9  29  \n",
            "1      POSITIVE  1995      9  29  \n",
            "2      POSITIVE  1995      9  29  \n",
            "3      POSITIVE  1995      9  29  \n",
            "4       NEUTRAL  2013     11  14  \n",
            "...         ...   ...    ...  ..  \n",
            "1322   POSITIVE  2008      4  22  \n",
            "1323   POSITIVE  2008      4  22  \n",
            "1324   POSITIVE  2008      4  22  \n",
            "1325   POSITIVE  2008      5  13  \n",
            "1326   POSITIVE  2008      5  13  \n",
            "\n",
            "[1327 rows x 8 columns]\n"
          ]
        }
      ],
      "source": [
        "# Your code here\n",
        "user_tampered = pd.merge(ChangeLog, tampered_df, on='title', how='inner')\n",
        "print(user_tampered)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRaNc7a89VVm"
      },
      "source": [
        "## 3.2 Identifying the users who have edited non-tampered documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "67nWYmT_BYIq"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "        UserID                                              title  \\\n",
            "0     [327047]  Tourists snap up British iPads to smuggle into...   \n",
            "1     [401818]  Tourists snap up British iPads to smuggle into...   \n",
            "2     [564061]  Tourists snap up British iPads to smuggle into...   \n",
            "3     [446376]  Tourists snap up British iPads to smuggle into...   \n",
            "4     [242912]           AFTER SEATTLE; Anarchists get organized.   \n",
            "...        ...                                                ...   \n",
            "1431  [484182]                                           IN BRIEF   \n",
            "1432  [220420]                                        Corrections   \n",
            "1433  [937535]                                        Corrections   \n",
            "1434  [428637]                                        Corrections   \n",
            "1435  [393146]                                        Corrections   \n",
            "\n",
            "          company                                               news  \\\n",
            "0          APPLE   IT'S the digital version of the slow boat to C...   \n",
            "1          APPLE   IT'S the digital version of the slow boat to C...   \n",
            "2          APPLE   IT'S the digital version of the slow boat to C...   \n",
            "3          APPLE   IT'S the digital version of the slow boat to C...   \n",
            "4         CHEVRON  For Juliette Beck, it began with the story of ...   \n",
            "...           ...                                                ...   \n",
            "1431  Exxon Mobil  WASHINGTON: NASA's Hubble Space Telescope has ...   \n",
            "1432      CHEVRON  An article on 7 18 about a lawsuit by New York...   \n",
            "1433      CHEVRON  An article on 7 18 about a lawsuit by New York...   \n",
            "1434      CHEVRON  An article on 7 18 about a lawsuit by New York...   \n",
            "1435      CHEVRON  An article on 7 18 about a lawsuit by New York...   \n",
            "\n",
            "     evaluation  year  month day  \n",
            "0      NEGATIVE  2011      4  17  \n",
            "1      NEGATIVE  2011      4  17  \n",
            "2      NEGATIVE  2011      4  17  \n",
            "3      NEGATIVE  2011      4  17  \n",
            "4      NEGATIVE  2000      4  17  \n",
            "...         ...   ...    ...  ..  \n",
            "1431   NEGATIVE  2008     12  11  \n",
            "1432   NEGATIVE  2007      7  28  \n",
            "1433   NEGATIVE  2007      7  28  \n",
            "1434   NEGATIVE  2007      7  28  \n",
            "1435   NEGATIVE  2007      7  28  \n",
            "\n",
            "[1436 rows x 8 columns]\n",
            "UserID [152304] found in the DataFrame.\n"
          ]
        }
      ],
      "source": [
        "# Your code here\n",
        "user_non_tampered = ChangeLog.merge(non_tampered_df, on='title', how='inner')\n",
        "print(user_non_tampered)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lrI3MKddC3UY"
      },
      "source": [
        "## 3.3 combining the results from `3.1` and `3.2` to identify users who only edited tampered documents.\n",
        "These are our suspects."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 176,
      "metadata": {
        "id": "cYl5RvF7DFcj"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "        UserID                                              title  \\\n",
            "8     [241404]  Letters: Anger simmers over energy bills; Thes...   \n",
            "13    [868229]  Hammer isn't enough to buy their mine rights; ...   \n",
            "17    [449172]  Run, don't walk, to greenhouse solutions; CAPI...   \n",
            "49    [425350]  BHP seeks greener ways to mine; As carbon tax ...   \n",
            "52    [231770]  Vast central Queensland LNG project may go int...   \n",
            "...        ...                                                ...   \n",
            "1248  [329120]     Rosneft and GE sign long-term service contract   \n",
            "1258  [354752]  Pay the price to secure the planet before it's...   \n",
            "1267  [806187]  Funds and pensions with $21 trillion pressure ...   \n",
            "1272  [648842]   Paw Prints Disappearing for Siberia's Amur Tiger   \n",
            "1278  [382851]  The greening of the computer; While the specia...   \n",
            "\n",
            "               company                                               news  \\\n",
            "8      BANK OF AMERICA  […] Ending our addiction to fossil fuels, slas...   \n",
            "13           CITIGROUP  Titus Natkime, 31, the son of a tribal leader ...   \n",
            "17           CITIGROUP  […] As is apparent from a report released rece...   \n",
            "49           CITIGROUP  BHP Billiton, the world's largest mining compa...   \n",
            "52           CITIGROUP  THE multibillion-dollar liquefied natural gas ...   \n",
            "...                ...                                                ...   \n",
            "1248  GENERAL ELECTRIC  Russia's state-run oil company Rosneft and GEN...   \n",
            "1258  GENERAL ELECTRIC  IMAGINE that, three or four years ago, a well-...   \n",
            "1267  GENERAL ELECTRIC  Investment funds and pensions with $21 trillio...   \n",
            "1272       Exxon Mobil  Over the last six years, Yury Dunishenko has w...   \n",
            "1278             INTEL  […] you want to own the world's most environme...   \n",
            "\n",
            "     evaluation  year  month day  \n",
            "8       NEUTRAL  2013     10  17  \n",
            "13      NEUTRAL  2006      4   1  \n",
            "17      NEUTRAL  2006     12  16  \n",
            "49      NEUTRAL  2007      6  19  \n",
            "52      NEUTRAL  2009      4   2  \n",
            "...         ...   ...    ...  ..  \n",
            "1248   POSITIVE  2012     10  31  \n",
            "1258   POSITIVE  2005      1  20  \n",
            "1267   POSITIVE  2005      9  19  \n",
            "1272   POSITIVE  2003     10   8  \n",
            "1278   POSITIVE  2007     11  15  \n",
            "\n",
            "[158 rows x 8 columns]\n",
            "UserID [527013] not found in the DataFrame.\n"
          ]
        }
      ],
      "source": [
        "# Your code here\n",
        "rows_only_in_tampered= user_tampered[~user_tampered['UserID'].isin(user_non_tampered['UserID'])]\n",
        "\n",
        "# Save the result to a new DataFrame\n",
        "result_df = rows_only_in_tampered\n",
        "\n",
        "# Print or do further processing with result_df\n",
        "print(result_df)\n",
        "\n",
        "target_user_id = '[527013]'\n",
        "\n",
        "# Check if the target_user_id exists in the UserID column\n",
        "if target_user_id in result_df['UserID'].values:\n",
        "    print(f\"UserID {target_user_id} found in the DataFrame.\")\n",
        "else:\n",
        "    print(f\"UserID {target_user_id} not found in the DataFrame.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3Zo8voPe-fP"
      },
      "source": [
        "# 4. Identifying important informations on the altered documents.\n",
        "\n",
        "In this section we will use the TF-IDF text representation model to identify other important information on the altered documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {
        "id": "FSPoVKNvTCIO"
      },
      "outputs": [],
      "source": [
        "# Make a list of the text within articles with the original dataset (the one of section 1)\n",
        "tempered_news_list = user_tampered['news'].tolist()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {
        "id": "pew__A22TA8r"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "      000  000km  000mwh   02   04  042   07   08   09  09bn  ...  zespri  \\\n",
            "0     0.0    0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  ...     0.0   \n",
            "1     0.0    0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  ...     0.0   \n",
            "2     0.0    0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  ...     0.0   \n",
            "3     0.0    0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  ...     0.0   \n",
            "4     0.0    0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  ...     0.0   \n",
            "...   ...    ...     ...  ...  ...  ...  ...  ...  ...   ...  ...     ...   \n",
            "1322  0.0    0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  ...     0.0   \n",
            "1323  0.0    0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  ...     0.0   \n",
            "1324  0.0    0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  ...     0.0   \n",
            "1325  0.0    0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  ...     0.0   \n",
            "1326  0.0    0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  ...     0.0   \n",
            "\n",
            "      zimbabwe  zip  zombo  zone  zones  zoning  zoo  zoologist  zurich  \n",
            "0          0.0  0.0    0.0   0.0    0.0     0.0  0.0        0.0     0.0  \n",
            "1          0.0  0.0    0.0   0.0    0.0     0.0  0.0        0.0     0.0  \n",
            "2          0.0  0.0    0.0   0.0    0.0     0.0  0.0        0.0     0.0  \n",
            "3          0.0  0.0    0.0   0.0    0.0     0.0  0.0        0.0     0.0  \n",
            "4          0.0  0.0    0.0   0.0    0.0     0.0  0.0        0.0     0.0  \n",
            "...        ...  ...    ...   ...    ...     ...  ...        ...     ...  \n",
            "1322       0.0  0.0    0.0   0.0    0.0     0.0  0.0        0.0     0.0  \n",
            "1323       0.0  0.0    0.0   0.0    0.0     0.0  0.0        0.0     0.0  \n",
            "1324       0.0  0.0    0.0   0.0    0.0     0.0  0.0        0.0     0.0  \n",
            "1325       0.0  0.0    0.0   0.0    0.0     0.0  0.0        0.0     0.0  \n",
            "1326       0.0  0.0    0.0   0.0    0.0     0.0  0.0        0.0     0.0  \n",
            "\n",
            "[1327 rows x 9894 columns]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1, 1))\n",
        "\n",
        "# Fit and transform the data to create the document-term matrix\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(tempered_news_list)\n",
        "\n",
        "# Get the feature names (vocabulary) used for the matrix\n",
        "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "\n",
        "# Convert the TF-IDF matrix to a DataFrame for visualization\n",
        "document_term_df = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)\n",
        "\n",
        "# Visualize the DataFrame\n",
        "print(document_term_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {
        "id": "Zo22PIjw1b13"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   word      value\n",
            "7791               said  43.256305\n",
            "1595             carbon  40.460089\n",
            "3249             energy  39.857131\n",
            "3999                gas  36.038164\n",
            "3195          emissions  30.574051\n",
            "...                 ...        ...\n",
            "344                 9pc   0.065978\n",
            "7753  ruckversicherungs   0.065978\n",
            "5971          munchener   0.065978\n",
            "1651        catchphrase   0.065978\n",
            "3521            express   0.065978\n",
            "\n",
            "[9894 rows x 2 columns]\n",
            "704\n"
          ]
        }
      ],
      "source": [
        "# Keep the entries related to tampered documents\n",
        "word_frequency = pd.DataFrame(document_term_df.sum()).reset_index()\n",
        "# Identify the record that stands out the most on the altered documents (ou can use the sum of the tokenizers results)\n",
        "word_frequency.columns = ['word', 'value']\n",
        "word_frequency = word_frequency.sort_values('value',ascending=False)\n",
        "print(word_frequency)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 167,
      "metadata": {
        "id": "485rSW0X4ShW"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "704\n",
            "424\n",
            "560\n",
            "449\n"
          ]
        }
      ],
      "source": [
        "# How many records contain the word that stands out the most?\n",
        "# e.g. if the word that stood out the most was \"mouton\", how many of the altered records contain the word mouton.\n",
        "print(document_term_df[document_term_df[word_frequency['word'].to_list()[0]] != 0].shape[0])\n",
        "# How about the second word that stands out the most\n",
        "second_most_common_word = word_frequency['word'].iloc[1]\n",
        "second_most_common_word_count = document_term_df[document_term_df[second_most_common_word] != 0].shape[0]\n",
        "print(second_most_common_word_count)\n",
        "# How about the third ?\n",
        "third_most_common_word = word_frequency['word'].iloc[2]\n",
        "third_most_common_word_count = document_term_df[document_term_df[third_most_common_word] != 0].shape[0]\n",
        "print(third_most_common_word_count)\n",
        "# How about the fourth ?\n",
        "fourth_most_common_word = word_frequency['word'].iloc[3]\n",
        "fourth_most_common_word_count = document_term_df[document_term_df[fourth_most_common_word] != 0].shape[0]\n",
        "print(fourth_most_common_word_count)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pK2MBdTr7KfK"
      },
      "source": [
        "#### Moodle quizz: if the order of frequency in appearance, did not match the values assigned by the tokenizer, is it normal?"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
